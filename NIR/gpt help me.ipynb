{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pems-bay.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle\n",
    "import folium\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('data/PEMS-BAY/pems-bay.h5', 'r') as file:\n",
    "\n",
    "    axis0 = file['speed']['axis0'][:]               # Идентификаторы датчиков\n",
    "    block0_items = file['speed']['block0_items'][:] # Идентификаторы датчиков\n",
    "    axis1 = file['speed']['axis1'][:]               # Метки времени\n",
    "    timestamps = pd.to_datetime(axis1)              # Преобразование меток времени в формат datetime\n",
    "    speed_data = file['speed']['block0_values'][:]  # Данные замеров скорости\n",
    "\n",
    "perms_bay = pd.DataFrame(speed_data, index=timestamps, columns=axis0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 52116 entries, 2017-01-01 00:00:00 to 2017-06-30 23:55:00\n",
      "Columns: 325 entries, 400001 to 414694\n",
      "dtypes: float64(325)\n",
      "memory usage: 129.6 MB\n"
     ]
    }
   ],
   "source": [
    "perms_bay.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Открытие .pkl файла\n",
    "with open('data/PEMS-BAY/adj_mx_bay.pkl', 'rb') as file:\n",
    "    data = pickle.load(file, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids = [x.decode('utf-8') for x in data[0]]                     # Получаем список id узлов из data[0]\n",
    "adj_matrix = data[2]                                                # Получаем матрицу смежности из data[2]\n",
    "adj_df = pd.DataFrame(adj_matrix, index=node_ids, columns=node_ids) # Создание DataFrame с использованием id узлов как индексов и названий колонок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_df = pd.read_csv('data/PEMS-BAY/distances_bay_2017.csv', header=None)\n",
    "locations_df = pd.read_csv('data/PEMS-BAY/graph_sensor_locations_bay.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_df.columns = ['from', 'to', 'distance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "class TrafficGraphData:\n",
    "    def __init__(self, adj_matrix, time_series, input_len, pred_len, train_ratio=0.7, val_ratio=0.15):\n",
    "        \"\"\"\n",
    "        Параметры:\n",
    "        - adj_matrix: pd.DataFrame — матрица смежности (размер N x N).\n",
    "        - time_series: pd.DataFrame — временные ряды (размер T x N).\n",
    "        - input_len: int — длина входного окна.\n",
    "        - pred_len: int — длина окна предсказания.\n",
    "        - train_ratio: float — доля данных для тренировки.\n",
    "        - val_ratio: float — доля данных для валидации.\n",
    "        \"\"\"\n",
    "        self.adj_matrix = adj_matrix.values.astype(np.float32)\n",
    "        self.time_series = time_series.values.astype(np.float32)\n",
    "        self.input_len = input_len\n",
    "        self.pred_len = pred_len\n",
    "        \n",
    "        # Нормализованная матрица Лапласа\n",
    "        self.norm_laplas = self._compute_norm_laplacian(self.adj_matrix)\n",
    "        \n",
    "        # Разделение данных\n",
    "        self.train_data, self.val_data, self.test_data = self._split_data(train_ratio, val_ratio)\n",
    "    \n",
    "    def _compute_norm_laplacian(self, adj_matrix):\n",
    "        \"\"\"\n",
    "        Вычисляет нормализованную матрицу Лапласа для графа.\n",
    "        \"\"\"\n",
    "        N = adj_matrix.shape[0]\n",
    "        adj_matrix += np.eye(N)  # Добавление self-loops\n",
    "        degree_matrix = np.diag(adj_matrix.sum(axis=1))\n",
    "        degree_inv_sqrt = np.linalg.inv(np.sqrt(degree_matrix))\n",
    "        laplacian = degree_inv_sqrt @ adj_matrix @ degree_inv_sqrt\n",
    "        return torch.tensor(laplacian, dtype=torch.float32)\n",
    "    \n",
    "    def _create_sliding_window(self, data):\n",
    "        \"\"\"\n",
    "        Создаёт скользящие окна для временных рядов.\n",
    "        Возвращает:\n",
    "        - X: np.array — входные данные (размер [кол-во окон, input_len, N]).\n",
    "        - Y: np.array — целевые значения (размер [кол-во окон, pred_len, N]).\n",
    "        \"\"\"\n",
    "        X, Y = [], []\n",
    "        for i in range(len(data) - self.input_len - self.pred_len + 1):\n",
    "            X.append(data[i : i + self.input_len])\n",
    "            Y.append(data[i + self.input_len : i + self.input_len + self.pred_len])\n",
    "        return np.array(X), np.array(Y)\n",
    "    \n",
    "    def _split_data(self, train_ratio, val_ratio):\n",
    "        \"\"\"\n",
    "        Делит временные данные на тренировочные, валидационные и тестовые наборы.\n",
    "        \"\"\"\n",
    "        X, Y = self._create_sliding_window(self.time_series)\n",
    "        total_len = len(X)\n",
    "        \n",
    "        train_size = int(train_ratio * total_len)\n",
    "        val_size = int(val_ratio * total_len)\n",
    "        \n",
    "        train_data = (X[:train_size], Y[:train_size])\n",
    "        val_data = (X[train_size:train_size + val_size], Y[train_size:train_size + val_size])\n",
    "        test_data = (X[train_size + val_size:], Y[train_size + val_size:])\n",
    "        \n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "    def get_train_data(self):\n",
    "        return self.train_data\n",
    "    \n",
    "    def get_val_data(self):\n",
    "        return self.val_data\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        return self.test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация класса\n",
    "input_len = 12       # Входное окно: 1 час (если шаг 5 минут)\n",
    "pred_len = 3         # Окно предсказания: 15 минут\n",
    "graph_data = TrafficGraphData(adj_df, perms_bay, input_len, pred_len)\n",
    "\n",
    "# Получение данных\n",
    "train_data = graph_data.get_train_data()\n",
    "val_data = graph_data.get_val_data()\n",
    "test_data = graph_data.get_test_data()\n",
    "\n",
    "# Матрица Лапласа\n",
    "laplacian_matrix = graph_data.norm_laplas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class TrafficGraphDataset:\n",
    "    def __init__(self, graph_data):\n",
    "        \"\"\"\n",
    "        Класс для создания датасета для GCN на основе подготовленных данных.\n",
    "        Параметры:\n",
    "        - graph_data: TrafficGraphData — объект с данными графа и временными рядами.\n",
    "        \"\"\"\n",
    "        self.train_data = graph_data.get_train_data()\n",
    "        self.val_data = graph_data.get_val_data()\n",
    "        self.test_data = graph_data.get_test_data()\n",
    "\n",
    "    def create_dataloader(self, data, batch_size):\n",
    "        \"\"\"\n",
    "        Создаёт DataLoader для удобной работы с данными.\n",
    "        \"\"\"\n",
    "        X, Y = data\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
    "        dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    def get_dataloaders(self, batch_size=32):\n",
    "        \"\"\"\n",
    "        Возвращает DataLoader'ы для тренировки, валидации и теста.\n",
    "        \"\"\"\n",
    "        train_loader = self.create_dataloader(self.train_data, batch_size)\n",
    "        val_loader = self.create_dataloader(self.val_data, batch_size)\n",
    "        test_loader = self.create_dataloader(self.test_data, batch_size)\n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class STGCN(nn.Module):\n",
    "    def __init__(self, laplacian, input_len, num_nodes, num_features, output_len, hidden_dim=64):\n",
    "        \"\"\"\n",
    "        Параметры:\n",
    "        - laplacian: torch.Tensor — нормализованная матрица Лапласа (\\( \\tilde{A} \\)).\n",
    "        - input_len: int — длина входного временного окна.\n",
    "        - num_nodes: int — количество узлов (N).\n",
    "        - num_features: int — число признаков на узел.\n",
    "        - output_len: int — длина окна предсказания.\n",
    "        - hidden_dim: int — размер скрытого слоя.\n",
    "        \"\"\"\n",
    "        super(STGCN, self).__init__()\n",
    "        \n",
    "        self.laplacian = laplacian\n",
    "        self.input_len = input_len\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_features = num_features\n",
    "        self.output_len = output_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # GCN слой\n",
    "        self.gcn = nn.Linear(num_features, hidden_dim)\n",
    "\n",
    "        # Временная свёртка\n",
    "        self.temporal_conv = nn.Conv2d(\n",
    "            in_channels=hidden_dim,\n",
    "            out_channels=hidden_dim,\n",
    "            kernel_size=(3, 1),  # Свёртка по времени\n",
    "            padding=(1, 0)  # Чтобы сохранить размер\n",
    "        )\n",
    "        \n",
    "        # Полносвязный слой для предсказания\n",
    "        self.fc = nn.Linear(hidden_dim, output_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: torch.Tensor — входной тензор [batch_size, input_len, num_nodes, num_features].\n",
    "        Возвращает: Предсказания [batch_size, output_len, num_nodes].\n",
    "        \"\"\"\n",
    "        if x.dim() == 3:  # Если [batch_size, input_len, num_nodes]\n",
    "            x = x.unsqueeze(-1)  # Добавляем размер признаков [batch_size, input_len, num_nodes, num_features]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "\n",
    "        # GCN операция: учёт связей между узлами\n",
    "        x = x.permute(0, 2, 1, 3)  # [batch_size, num_nodes, input_len, num_features]\n",
    "        x = x.reshape(-1, self.num_features)  # [batch_size * num_nodes, num_features]\n",
    "        x = F.relu(torch.matmul(self.laplacian, self.gcn(x)))  # [batch_size * num_nodes, hidden_dim]\n",
    "        x = x.view(batch_size, self.num_nodes, self.input_len, self.hidden_dim)\n",
    "        x = x.permute(0, 3, 2, 1)  # [batch_size, hidden_dim, input_len, num_nodes]\n",
    "\n",
    "        # Временная свёртка\n",
    "        x = F.relu(self.temporal_conv(x))  # [batch_size, hidden_dim, input_len, num_nodes]\n",
    "\n",
    "        # Сжатие по времени и выход\n",
    "        x = x.mean(dim=2)  # Убираем временную ось [batch_size, hidden_dim, num_nodes]\n",
    "        x = self.fc(x)  # [batch_size, num_nodes, output_len]\n",
    "        x = x.permute(0, 2, 1)  # [batch_size, output_len, num_nodes]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры модели\n",
    "input_len = 12       # Входное окно: 12 шагов\n",
    "pred_len = 3         # Выходное окно: 3 шага\n",
    "num_nodes = adj_df.shape[0]\n",
    "num_features = 1     # Одно значение на узел (например, поток)\n",
    "hidden_dim = 64\n",
    "\n",
    "# Инициализация модели\n",
    "laplacian = graph_data.norm_laplas\n",
    "model = STGCN(laplacian, input_len, num_nodes, num_features, pred_len, hidden_dim)\n",
    "\n",
    "# Пример данных\n",
    "X_train, Y_train = graph_data.get_train_data()\n",
    "X_train_tensor = torch.tensor(X_train[:1, :, :], dtype=torch.float32)  # [batch_size, input_len, num_nodes, num_features]\n",
    "Y_train_tensor = torch.tensor(Y_train[:1, :, :], dtype=torch.float32)  # [batch_size, pred_len, num_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_train_tensor.shape = torch.Size([1, 12, 325])'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{X_train_tensor.shape = }\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (325x325 and 3900x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Прямой проход\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Ожидаем: [batch_size, pred_len, num_nodes]\u001b[39;00m\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[14], line 52\u001b[0m, in \u001b[0;36mSTGCN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     50\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# [batch_size, num_nodes, input_len, num_features]\u001b[39;00m\n\u001b[0;32m     51\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features)  \u001b[38;5;66;03m# [batch_size * num_nodes, num_features]\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaplacian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# [batch_size * num_nodes, hidden_dim]\u001b[39;00m\n\u001b[0;32m     53\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim)\n\u001b[0;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size, hidden_dim, input_len, num_nodes]\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (325x325 and 3900x64)"
     ]
    }
   ],
   "source": [
    "# Прямой проход\n",
    "predictions = model(X_train_tensor)\n",
    "print(predictions.shape)  # Ожидаем: [batch_size, pred_len, num_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Input shape after adding feature dim: torch.Size([1, 12, 325, 1])\n",
      "Step 2 - Shape after permute: torch.Size([1, 325, 12, 1])\n",
      "Step 3 - Shape after reshaping for GCN: torch.Size([325, 12, 1])\n",
      "Step 4 - Shape after GCN layer: torch.Size([325, 12, 64])\n",
      "Step 5 - Shape after Laplacian multiplication: torch.Size([1, 325, 12, 64])\n",
      "Step 6 - Shape after temporal convolution: torch.Size([1, 64, 12, 325])\n",
      "Step 7 - Shape after reducing along time axis: torch.Size([1, 64, 325])\n",
      "Step 8 - Final prediction shape: torch.Size([1, 3, 325])\n",
      "Predictions shape: torch.Size([1, 3, 325])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class STGCN(nn.Module):\n",
    "    def __init__(self, laplacian, input_len, num_nodes, num_features, output_len, hidden_dim=64):\n",
    "        super(STGCN, self).__init__()\n",
    "        \n",
    "        self.laplacian = laplacian  # [num_nodes, num_nodes]\n",
    "        self.input_len = input_len\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_features = num_features\n",
    "        self.output_len = output_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # GCN layer\n",
    "        self.gcn = nn.Linear(num_features, hidden_dim)\n",
    "\n",
    "        # Temporal convolution\n",
    "        self.temporal_conv = nn.Conv2d(\n",
    "            in_channels=hidden_dim,\n",
    "            out_channels=hidden_dim,\n",
    "            kernel_size=(3, 1),  # Convolution along time\n",
    "            padding=(1, 0)  # Maintain size\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer for prediction\n",
    "        self.fc = nn.Linear(hidden_dim, output_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: torch.Tensor — input tensor [batch_size, input_len, num_nodes, num_features].\n",
    "        Returns: Predictions [batch_size, output_len, num_nodes].\n",
    "        \"\"\"\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(-1)  # Add feature dimension [batch_size, input_len, num_nodes, num_features]\n",
    "        print(f\"Step 1 - Input shape after adding feature dim: {x.shape}\")\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # GCN operation for each time step\n",
    "        x = x.permute(0, 2, 1, 3)  # [batch_size, num_nodes, input_len, num_features]\n",
    "        print(f\"Step 2 - Shape after permute: {x.shape}\")\n",
    "        x = x.reshape(batch_size * self.num_nodes, self.input_len, self.num_features)  # [batch_size * num_nodes, input_len, num_features]\n",
    "        print(f\"Step 3 - Shape after reshaping for GCN: {x.shape}\")\n",
    "        \n",
    "        x = F.relu(self.gcn(x))  # Apply GCN layer: [batch_size * num_nodes, input_len, hidden_dim]\n",
    "        print(f\"Step 4 - Shape after GCN layer: {x.shape}\")\n",
    "        \n",
    "        # Apply Laplacian to each time step\n",
    "        x = x.view(batch_size, self.num_nodes, self.input_len, self.hidden_dim)  # [batch_size, num_nodes, input_len, hidden_dim]\n",
    "        x = torch.einsum('ij,bjtl->bitl', self.laplacian, x)  # Graph convolution using Laplacian: [batch_size, num_nodes, input_len, hidden_dim]\n",
    "        print(f\"Step 5 - Shape after Laplacian multiplication: {x.shape}\")\n",
    "\n",
    "        # Temporal convolution\n",
    "        x = x.permute(0, 3, 2, 1)  # [batch_size, hidden_dim, input_len, num_nodes]\n",
    "        x = F.relu(self.temporal_conv(x))  # [batch_size, hidden_dim, input_len, num_nodes]\n",
    "        print(f\"Step 6 - Shape after temporal convolution: {x.shape}\")\n",
    "\n",
    "        # Reduce along the time axis\n",
    "        x = x.mean(dim=2)  # Remove time axis [batch_size, hidden_dim, num_nodes]\n",
    "        print(f\"Step 7 - Shape after reducing along time axis: {x.shape}\")\n",
    "        \n",
    "        x = x.permute(0, 2, 1)  # [batch_size, num_nodes, hidden_dim]\n",
    "        x = self.fc(x)  # [batch_size, num_nodes, output_len]\n",
    "        x = x.permute(0, 2, 1)  # [batch_size, output_len, num_nodes]\n",
    "        print(f\"Step 8 - Final prediction shape: {x.shape}\")\n",
    "\n",
    "        return x\n",
    "\n",
    "# Тестируем модель\n",
    "laplacian = torch.eye(325)  # Identity matrix for testing\n",
    "input_len = 12\n",
    "num_nodes = 325\n",
    "num_features = 1\n",
    "output_len = 3\n",
    "hidden_dim = 64\n",
    "\n",
    "# Данные\n",
    "X_train_tensor = torch.randn(1, 12, 325)  # Simulated input data\n",
    "\n",
    "# Инициализируем модель\n",
    "model = STGCN(laplacian, input_len, num_nodes, num_features, output_len, hidden_dim)\n",
    "\n",
    "# Forward pass\n",
    "predictions = model(X_train_tensor)\n",
    "print(f\"Predictions shape: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "      <th>321</th>\n",
       "      <th>322</th>\n",
       "      <th>323</th>\n",
       "      <th>324</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022213</td>\n",
       "      <td>0.009949</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.015424</td>\n",
       "      <td>0.023195</td>\n",
       "      <td>0.041739</td>\n",
       "      <td>-0.014152</td>\n",
       "      <td>0.016560</td>\n",
       "      <td>0.025389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007506</td>\n",
       "      <td>0.023157</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>0.016188</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>0.009272</td>\n",
       "      <td>0.006271</td>\n",
       "      <td>-0.005405</td>\n",
       "      <td>-0.001155</td>\n",
       "      <td>0.031597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.065086</td>\n",
       "      <td>0.036740</td>\n",
       "      <td>0.044935</td>\n",
       "      <td>0.055187</td>\n",
       "      <td>0.058353</td>\n",
       "      <td>0.064074</td>\n",
       "      <td>0.050105</td>\n",
       "      <td>0.033043</td>\n",
       "      <td>0.054188</td>\n",
       "      <td>0.063349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051942</td>\n",
       "      <td>0.055016</td>\n",
       "      <td>0.063394</td>\n",
       "      <td>0.056493</td>\n",
       "      <td>0.057803</td>\n",
       "      <td>0.051921</td>\n",
       "      <td>0.057320</td>\n",
       "      <td>0.053621</td>\n",
       "      <td>0.043851</td>\n",
       "      <td>0.041138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.207885</td>\n",
       "      <td>0.228600</td>\n",
       "      <td>0.217451</td>\n",
       "      <td>0.216187</td>\n",
       "      <td>0.212961</td>\n",
       "      <td>0.226008</td>\n",
       "      <td>0.221211</td>\n",
       "      <td>0.210272</td>\n",
       "      <td>0.226671</td>\n",
       "      <td>0.231599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218479</td>\n",
       "      <td>0.237429</td>\n",
       "      <td>0.203693</td>\n",
       "      <td>0.216754</td>\n",
       "      <td>0.220643</td>\n",
       "      <td>0.213421</td>\n",
       "      <td>0.209715</td>\n",
       "      <td>0.213363</td>\n",
       "      <td>0.243100</td>\n",
       "      <td>0.247521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.022213  0.009949  0.019647  0.000767  0.015424  0.023195  0.041739   \n",
       "1  0.065086  0.036740  0.044935  0.055187  0.058353  0.064074  0.050105   \n",
       "2  0.207885  0.228600  0.217451  0.216187  0.212961  0.226008  0.221211   \n",
       "\n",
       "        7         8         9    ...       315       316       317       318  \\\n",
       "0 -0.014152  0.016560  0.025389  ...  0.007506  0.023157  0.003008  0.016188   \n",
       "1  0.033043  0.054188  0.063349  ...  0.051942  0.055016  0.063394  0.056493   \n",
       "2  0.210272  0.226671  0.231599  ...  0.218479  0.237429  0.203693  0.216754   \n",
       "\n",
       "        319       320       321       322       323       324  \n",
       "0  0.001956  0.009272  0.006271 -0.005405 -0.001155  0.031597  \n",
       "1  0.057803  0.051921  0.057320  0.053621  0.043851  0.041138  \n",
       "2  0.220643  0.213421  0.209715  0.213363  0.243100  0.247521  \n",
       "\n",
       "[3 rows x 325 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(predictions[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.dataset import METRLADatasetLoader, PemsBayDatasetLoader\n",
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "\n",
    "# Загрузка данных\n",
    "loader = PemsBayDatasetLoader()\n",
    "dataset = loader.get_dataset(12, 3)\n",
    "\n",
    "# Разделение на train и test\n",
    "train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([325, 2, 12]) torch.Size([325, 2, 3]) torch.Size([2, 2694])\n"
     ]
    }
   ],
   "source": [
    "for snapshot in dataset:\n",
    "    print(snapshot.x.shape, snapshot.y.shape, snapshot.edge_index.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.nn.recurrent import GConvGRU\n",
    "from torch_geometric_temporal.nn.attention.gman import SpatialAttention, TemporalAttention\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class TrafficPredictionModel(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.gconvgru = GConvGRU(input_dim, hidden_dim, num_nodes)\n",
    "        self.spatial_attention = SpatialAttention(num_nodes, hidden_dim, bn_decay=0.9)\n",
    "        self.temporal_attention = TemporalAttention(d=hidden_dim, K=4, bn_decay=0.9, mask=False)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        # Рекуррентный слой\n",
    "        h = self.gconvgru(x, edge_index, edge_weight)\n",
    "        \n",
    "        # Пространственное внимание\n",
    "        h = self.spatial_attention(h)\n",
    "        \n",
    "        # Временное внимание\n",
    "        h = self.temporal_attention(h)\n",
    "        \n",
    "        # Полносвязный слой\n",
    "        out = self.fc(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     62\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, y_train)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[88], line 22\u001b[0m, in \u001b[0;36mTrafficPredictionModel.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, edge_weight):\n\u001b[1;32m---> 22\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgconvgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_attention(h)\n\u001b[0;32m     24\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_attention(h)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric_temporal\\nn\\recurrent\\gconv_gru.py:166\u001b[0m, in \u001b[0;36mGConvGRU.forward\u001b[1;34m(self, X, edge_index, edge_weight, H, lambda_max)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03mMaking a forward pass. If edge weights are not present the forward pass\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03mdefaults to an unweighted graph. If the hidden state matrix is not present\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    * **H** *(PyTorch Float Tensor)* - Hidden state matrix for all nodes.\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    165\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_hidden_state(X, H)\n\u001b[1;32m--> 166\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_update_gate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m R \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_reset_gate(X, edge_index, edge_weight, H, lambda_max)\n\u001b[0;32m    168\u001b[0m H_tilde \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_candidate_state(X, edge_index, edge_weight, H, R, lambda_max)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric_temporal\\nn\\recurrent\\gconv_gru.py:120\u001b[0m, in \u001b[0;36mGConvGRU._calculate_update_gate\u001b[1;34m(self, X, edge_index, edge_weight, H, lambda_max)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_calculate_update_gate\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, edge_index, edge_weight, H, lambda_max):\n\u001b[1;32m--> 120\u001b[0m     Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_x_z\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     Z \u001b[38;5;241m=\u001b[39m Z \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_h_z(H, edge_index, edge_weight, lambda_max\u001b[38;5;241m=\u001b[39mlambda_max)\n\u001b[0;32m    122\u001b[0m     Z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(Z)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric\\nn\\conv\\cheb_conv.py:151\u001b[0m, in \u001b[0;36mChebConv.forward\u001b[1;34m(self, x, edge_index, edge_weight, batch, lambda_max)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    144\u001b[0m     x: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    148\u001b[0m     lambda_max: OptTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    149\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 151\u001b[0m     edge_index, norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__norm__\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlambda_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     Tx_0 \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    162\u001b[0m     Tx_1 \u001b[38;5;241m=\u001b[39m x  \u001b[38;5;66;03m# Dummy.\u001b[39;00m\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric\\nn\\conv\\cheb_conv.py:119\u001b[0m, in \u001b[0;36mChebConv.__norm__\u001b[1;34m(self, edge_index, num_nodes, edge_weight, normalization, lambda_max, dtype, batch)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__norm__\u001b[39m(\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    111\u001b[0m     edge_index: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m     batch: OptTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    118\u001b[0m ):\n\u001b[1;32m--> 119\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mget_laplacian\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mnormalization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lambda_max \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric\\utils\\laplacian.py:69\u001b[0m, in \u001b[0;36mget_laplacian\u001b[1;34m(edge_index, edge_weight, normalization, dtype, num_nodes)\u001b[0m\n\u001b[0;32m     66\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[0;32m     68\u001b[0m row, col \u001b[38;5;241m=\u001b[39m edge_index[\u001b[38;5;241m0\u001b[39m], edge_index[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 69\u001b[0m deg \u001b[38;5;241m=\u001b[39m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# L = D - A.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     edge_index, _ \u001b[38;5;241m=\u001b[39m add_self_loops(edge_index, num_nodes\u001b[38;5;241m=\u001b[39mnum_nodes)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric\\utils\\_scatter.py:75\u001b[0m, in \u001b[0;36mscatter\u001b[1;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     74\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     78\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: index 2 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric_temporal.dataset import PemsBayDatasetLoader\n",
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "# # Load the dataset\n",
    "# loader = PemsBayDatasetLoader()\n",
    "# dataset = loader.get_dataset(12, 3)  # 12 timesteps input, 3 timesteps output\n",
    "# train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)\n",
    "\n",
    "# Prepare the model\n",
    "class TrafficPredictionModel(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.gconvgru = GConvGRU(input_dim, hidden_dim, num_nodes)\n",
    "        self.spatial_attention = SpatialAttention(num_nodes, hidden_dim, bn_decay=0.9)\n",
    "        self.temporal_attention = TemporalAttention(d=hidden_dim, K=4, bn_decay=0.9, mask=False)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        h = self.gconvgru(x, edge_index, edge_weight)\n",
    "        h = self.spatial_attention(h)\n",
    "        h = self.temporal_attention(h)\n",
    "        out = self.fc(h)\n",
    "        return out\n",
    "\n",
    "# Data Preparation Function\n",
    "def prepare_data_for_model(dataset):\n",
    "    # Extract key components\n",
    "    x = torch.tensor(dataset.features, dtype=torch.float)  # Node features\n",
    "    edge_index = torch.tensor(dataset.edge_index, dtype=torch.long)  # Graph connectivity\n",
    "    edge_weight = torch.tensor(dataset.edge_weight, dtype=torch.float)  # Edge weights\n",
    "    y = torch.tensor(dataset.targets, dtype=torch.float)  # Target values\n",
    "\n",
    "    return x, edge_index, edge_weight, y\n",
    "\n",
    "# Prepare training and test data\n",
    "x_train, edge_index_train, edge_weight_train, y_train = prepare_data_for_model(train_dataset)\n",
    "x_test, edge_index_test, edge_weight_test, y_test = prepare_data_for_model(test_dataset)\n",
    "\n",
    "# Instantiate the model\n",
    "num_nodes = x_train.shape[1]  # Number of nodes\n",
    "input_dim = x_train.shape[2]  # Input feature dimension\n",
    "hidden_dim = 64  # You can adjust this\n",
    "output_dim = y_train.shape[2]  # Output feature dimension\n",
    "\n",
    "model = TrafficPredictionModel(num_nodes, input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Training loop (example)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):  # Number of epochs\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x_train, edge_index_train, edge_weight_train)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(output, y_train)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_output = model(x_test, edge_index_test, edge_weight_test)\n",
    "    test_loss = criterion(test_output, y_test)\n",
    "    print(f'Test Loss: {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры модели\n",
    "num_nodes = 325  # Количество узлов в графе\n",
    "input_dim = 2    # Размерность входных признаков\n",
    "hidden_dim = 64  # Размерность скрытого состояния\n",
    "output_dim = 3   # Размерность выхода\n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch):\n",
    "    # Преобразование формата входных данных [N, F, T] -> [N, T, F]\n",
    "    x = batch.x.permute(0, 2, 1)\n",
    "    \n",
    "    # Получение edge_index и edge_weight\n",
    "    edge_index = batch.edge_index\n",
    "    edge_weight = batch.edge_attr\n",
    "    \n",
    "    return x, edge_index, edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "model = TrafficPredictionModel(num_nodes, input_dim, hidden_dim, output_dim)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41672"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 1/10:   0%|          | 0/41672 [00:00<?, ?it/s]C:\\Users\\romab\\AppData\\Local\\Temp\\ipykernel_3760\\1753902722.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "d:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Эпоха 1/10:   0%|          | 0/41672 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "index 12 is out of bounds for dimension 0 with size 12",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;66;03m# Очистка памяти в конце эпохи    \u001b[39;00m\n\u001b[0;32m     54\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m---> 56\u001b[0m \u001b[43mtrain_with_gradient_accumulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[85], line 36\u001b[0m, in \u001b[0;36mtrain_with_gradient_accumulation\u001b[1;34m(model, train_dataset, optimizer, epochs, accumulation_steps)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Прямой проход с использованием half precision\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[1;32m---> 36\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(out, y)\n\u001b[0;32m     38\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m accumulation_steps\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[60], line 16\u001b[0m, in \u001b[0;36mTrafficPredictionModel.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, edge_weight):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Рекуррентный слой\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgconvgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Пространственное внимание\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_attention(h)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric_temporal\\nn\\recurrent\\gconv_gru.py:166\u001b[0m, in \u001b[0;36mGConvGRU.forward\u001b[1;34m(self, X, edge_index, edge_weight, H, lambda_max)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03mMaking a forward pass. If edge weights are not present the forward pass\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03mdefaults to an unweighted graph. If the hidden state matrix is not present\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    * **H** *(PyTorch Float Tensor)* - Hidden state matrix for all nodes.\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    165\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_hidden_state(X, H)\n\u001b[1;32m--> 166\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_update_gate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m R \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_reset_gate(X, edge_index, edge_weight, H, lambda_max)\n\u001b[0;32m    168\u001b[0m H_tilde \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_candidate_state(X, edge_index, edge_weight, H, R, lambda_max)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric_temporal\\nn\\recurrent\\gconv_gru.py:120\u001b[0m, in \u001b[0;36mGConvGRU._calculate_update_gate\u001b[1;34m(self, X, edge_index, edge_weight, H, lambda_max)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_calculate_update_gate\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, edge_index, edge_weight, H, lambda_max):\n\u001b[1;32m--> 120\u001b[0m     Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_x_z\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     Z \u001b[38;5;241m=\u001b[39m Z \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_h_z(H, edge_index, edge_weight, lambda_max\u001b[38;5;241m=\u001b[39mlambda_max)\n\u001b[0;32m    122\u001b[0m     Z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(Z)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric\\nn\\conv\\cheb_conv.py:151\u001b[0m, in \u001b[0;36mChebConv.forward\u001b[1;34m(self, x, edge_index, edge_weight, batch, lambda_max)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    144\u001b[0m     x: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    148\u001b[0m     lambda_max: OptTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    149\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 151\u001b[0m     edge_index, norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__norm__\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlambda_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     Tx_0 \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    162\u001b[0m     Tx_1 \u001b[38;5;241m=\u001b[39m x  \u001b[38;5;66;03m# Dummy.\u001b[39;00m\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric\\nn\\conv\\cheb_conv.py:119\u001b[0m, in \u001b[0;36mChebConv.__norm__\u001b[1;34m(self, edge_index, num_nodes, edge_weight, normalization, lambda_max, dtype, batch)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__norm__\u001b[39m(\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    111\u001b[0m     edge_index: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m     batch: OptTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    118\u001b[0m ):\n\u001b[1;32m--> 119\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mget_laplacian\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mnormalization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lambda_max \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric\\utils\\laplacian.py:69\u001b[0m, in \u001b[0;36mget_laplacian\u001b[1;34m(edge_index, edge_weight, normalization, dtype, num_nodes)\u001b[0m\n\u001b[0;32m     66\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[0;32m     68\u001b[0m row, col \u001b[38;5;241m=\u001b[39m edge_index[\u001b[38;5;241m0\u001b[39m], edge_index[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 69\u001b[0m deg \u001b[38;5;241m=\u001b[39m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# L = D - A.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     edge_index, _ \u001b[38;5;241m=\u001b[39m add_self_loops(edge_index, num_nodes\u001b[38;5;241m=\u001b[39mnum_nodes)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric\\utils\\_scatter.py:75\u001b[0m, in \u001b[0;36mscatter\u001b[1;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     74\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     78\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: index 12 is out of bounds for dimension 0 with size 12"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Включаем оптимизацию памяти\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "len_train_dataset = len(list(train_dataset))\n",
    "\n",
    "# Функция для обработки батча с градиентным накоплением\n",
    "def train_with_gradient_accumulation(model, train_dataset, optimizer, epochs, \n",
    "                                   accumulation_steps=4):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Создаем tqdm с описанием прогресса\n",
    "        pbar = tqdm(enumerate(train_dataset), \n",
    "                   total=len_train_dataset,\n",
    "                   desc=f'Эпоха {epoch+1}/{epochs}',\n",
    "                   leave=True)\n",
    "        \n",
    "        for idx, batch in pbar:\n",
    "            # Перемещаем данные на устройство\n",
    "            x, edge_index, edge_weight = process_batch(batch)\n",
    "            x = x.to(device)\n",
    "            edge_index = edge_index.to(device)\n",
    "            edge_weight = edge_weight.to(device)\n",
    "            y = batch.y.to(device)\n",
    "            \n",
    "            # Прямой проход с использованием half precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out = model(x, edge_index, edge_weight)\n",
    "                loss = F.mse_loss(out, y)\n",
    "                loss = loss / accumulation_steps\n",
    "            \n",
    "            # Обратное распространение\n",
    "            loss.backward()\n",
    "            \n",
    "            # Обновление весов после накопления градиентов\n",
    "            if (idx + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Обновление прогресс-бара\n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "            avg_loss = total_loss / (idx + 1)\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
    "            \n",
    "        # Очистка памяти в конце эпохи    \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "train_with_gradient_accumulation(model, train_dataset, optimizer, epochs, accumulation_steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3461120000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Обучение модели\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTrafficPredictionModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "Cell \u001b[1;32mIn[60], line 10\u001b[0m, in \u001b[0;36mTrafficPredictionModel.__init__\u001b[1;34m(self, num_nodes, input_dim, hidden_dim, output_dim)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgconvgru \u001b[38;5;241m=\u001b[39m GConvGRU(input_dim, hidden_dim, num_nodes)\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_attention \u001b[38;5;241m=\u001b[39m \u001b[43mSpatialAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbn_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_attention \u001b[38;5;241m=\u001b[39m TemporalAttention(d\u001b[38;5;241m=\u001b[39mhidden_dim, K\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, bn_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(hidden_dim, output_dim)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric_temporal\\nn\\attention\\gman.py:210\u001b[0m, in \u001b[0;36mSpatialAttention.__init__\u001b[1;34m(self, K, d, bn_decay)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fully_connected_q \u001b[38;5;241m=\u001b[39m FullyConnected(\n\u001b[0;32m    205\u001b[0m     input_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m D, units\u001b[38;5;241m=\u001b[39mD, activations\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mrelu, bn_decay\u001b[38;5;241m=\u001b[39mbn_decay\n\u001b[0;32m    206\u001b[0m )\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fully_connected_k \u001b[38;5;241m=\u001b[39m FullyConnected(\n\u001b[0;32m    208\u001b[0m     input_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m D, units\u001b[38;5;241m=\u001b[39mD, activations\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mrelu, bn_decay\u001b[38;5;241m=\u001b[39mbn_decay\n\u001b[0;32m    209\u001b[0m )\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fully_connected_v \u001b[38;5;241m=\u001b[39m \u001b[43mFullyConnected\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbn_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbn_decay\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fully_connected \u001b[38;5;241m=\u001b[39m FullyConnected(\n\u001b[0;32m    214\u001b[0m     input_dims\u001b[38;5;241m=\u001b[39mD, units\u001b[38;5;241m=\u001b[39mD, activations\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mrelu, bn_decay\u001b[38;5;241m=\u001b[39mbn_decay\n\u001b[0;32m    215\u001b[0m )\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric_temporal\\nn\\attention\\gman.py:96\u001b[0m, in \u001b[0;36mFullyConnected.__init__\u001b[1;34m(self, input_dims, units, activations, bn_decay, use_bias)\u001b[0m\n\u001b[0;32m     93\u001b[0m     activations \u001b[38;5;241m=\u001b[39m [activations]\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(units) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv2ds \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m---> 96\u001b[0m     [\n\u001b[0;32m     97\u001b[0m         Conv2D(\n\u001b[0;32m     98\u001b[0m             input_dims\u001b[38;5;241m=\u001b[39minput_dim,\n\u001b[0;32m     99\u001b[0m             output_dims\u001b[38;5;241m=\u001b[39mnum_unit,\n\u001b[0;32m    100\u001b[0m             kernel_size\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    101\u001b[0m             stride\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    102\u001b[0m             use_bias\u001b[38;5;241m=\u001b[39muse_bias,\n\u001b[0;32m    103\u001b[0m             activation\u001b[38;5;241m=\u001b[39mactivation,\n\u001b[0;32m    104\u001b[0m             bn_decay\u001b[38;5;241m=\u001b[39mbn_decay,\n\u001b[0;32m    105\u001b[0m         )\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m input_dim, num_unit, activation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[0;32m    107\u001b[0m             input_dims, units, activations\n\u001b[0;32m    108\u001b[0m         )\n\u001b[0;32m    109\u001b[0m     ]\n\u001b[0;32m    110\u001b[0m )\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric_temporal\\nn\\attention\\gman.py:97\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     93\u001b[0m     activations \u001b[38;5;241m=\u001b[39m [activations]\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(units) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv2ds \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m     96\u001b[0m     [\n\u001b[1;32m---> 97\u001b[0m         \u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m            \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbn_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbn_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m input_dim, num_unit, activation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[0;32m    107\u001b[0m             input_dims, units, activations\n\u001b[0;32m    108\u001b[0m         )\n\u001b[0;32m    109\u001b[0m     ]\n\u001b[0;32m    110\u001b[0m )\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric_temporal\\nn\\attention\\gman.py:36\u001b[0m, in \u001b[0;36mConv2D.__init__\u001b[1;34m(self, input_dims, output_dims, kernel_size, stride, use_bias, activation, bn_decay)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28msuper\u001b[39m(Conv2D, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activation \u001b[38;5;241m=\u001b[39m activation\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv2d \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBatchNorm2d(output_dims, momentum\u001b[38;5;241m=\u001b[39mbn_decay)\n\u001b[0;32m     45\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mxavier_uniform_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv2d\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:521\u001b[0m, in \u001b[0;36mConv2d.__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m    519\u001b[0m padding_ \u001b[38;5;241m=\u001b[39m padding \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _pair(padding)\n\u001b[0;32m    520\u001b[0m dilation_ \u001b[38;5;241m=\u001b[39m _pair(dilation)\n\u001b[1;32m--> 521\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    522\u001b[0m     in_channels,\n\u001b[0;32m    523\u001b[0m     out_channels,\n\u001b[0;32m    524\u001b[0m     kernel_size_,\n\u001b[0;32m    525\u001b[0m     stride_,\n\u001b[0;32m    526\u001b[0m     padding_,\n\u001b[0;32m    527\u001b[0m     dilation_,\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    529\u001b[0m     _pair(\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m    530\u001b[0m     groups,\n\u001b[0;32m    531\u001b[0m     bias,\n\u001b[0;32m    532\u001b[0m     padding_mode,\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs,\n\u001b[0;32m    534\u001b[0m )\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:166\u001b[0m, in \u001b[0;36m_ConvNd.__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\n\u001b[0;32m    159\u001b[0m         torch\u001b[38;5;241m.\u001b[39mempty(\n\u001b[0;32m    160\u001b[0m             (in_channels, out_channels \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m groups, \u001b[38;5;241m*\u001b[39mkernel_size),\n\u001b[0;32m    161\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs,\n\u001b[0;32m    162\u001b[0m         )\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\n\u001b[1;32m--> 166\u001b[0m         torch\u001b[38;5;241m.\u001b[39mempty(\n\u001b[0;32m    167\u001b[0m             (out_channels, in_channels \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m groups, \u001b[38;5;241m*\u001b[39mkernel_size),\n\u001b[0;32m    168\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs,\n\u001b[0;32m    169\u001b[0m         )\n\u001b[0;32m    170\u001b[0m     )\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_channels, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3461120000 bytes."
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "# Обучение модели\n",
    "model = TrafficPredictionModel(num_nodes, input_dim, hidden_dim, output_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataset):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Подготовка данных\n",
    "        x, edge_index, edge_weight = process_batch(batch)\n",
    "        y = batch.y\n",
    "        \n",
    "        # Прямой проход\n",
    "        out = model(x, edge_index, edge_weight)\n",
    "        \n",
    "        # Расчет потерь\n",
    "        loss = F.mse_loss(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Step 6: Train the model\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[70], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dataset, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[0;32m     14\u001b[0m edge_index, edge_weight \u001b[38;5;241m=\u001b[39m add_self_loops(edge_index, edge_attr\u001b[38;5;241m=\u001b[39medge_weight, num_nodes\u001b[38;5;241m=\u001b[39mnum_nodes)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[60], line 16\u001b[0m, in \u001b[0;36mTrafficPredictionModel.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, edge_weight):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Рекуррентный слой\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgconvgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Пространственное внимание\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_attention(h)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric_temporal\\nn\\recurrent\\gconv_gru.py:166\u001b[0m, in \u001b[0;36mGConvGRU.forward\u001b[1;34m(self, X, edge_index, edge_weight, H, lambda_max)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03mMaking a forward pass. If edge weights are not present the forward pass\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03mdefaults to an unweighted graph. If the hidden state matrix is not present\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    * **H** *(PyTorch Float Tensor)* - Hidden state matrix for all nodes.\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    165\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_hidden_state(X, H)\n\u001b[1;32m--> 166\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_update_gate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m R \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_reset_gate(X, edge_index, edge_weight, H, lambda_max)\n\u001b[0;32m    168\u001b[0m H_tilde \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_candidate_state(X, edge_index, edge_weight, H, R, lambda_max)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric_temporal\\nn\\recurrent\\gconv_gru.py:120\u001b[0m, in \u001b[0;36mGConvGRU._calculate_update_gate\u001b[1;34m(self, X, edge_index, edge_weight, H, lambda_max)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_calculate_update_gate\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, edge_index, edge_weight, H, lambda_max):\n\u001b[1;32m--> 120\u001b[0m     Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_x_z\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     Z \u001b[38;5;241m=\u001b[39m Z \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_h_z(H, edge_index, edge_weight, lambda_max\u001b[38;5;241m=\u001b[39mlambda_max)\n\u001b[0;32m    122\u001b[0m     Z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(Z)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric\\nn\\conv\\cheb_conv.py:151\u001b[0m, in \u001b[0;36mChebConv.forward\u001b[1;34m(self, x, edge_index, edge_weight, batch, lambda_max)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    144\u001b[0m     x: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    148\u001b[0m     lambda_max: OptTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    149\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 151\u001b[0m     edge_index, norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__norm__\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlambda_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     Tx_0 \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    162\u001b[0m     Tx_1 \u001b[38;5;241m=\u001b[39m x  \u001b[38;5;66;03m# Dummy.\u001b[39;00m\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric\\nn\\conv\\cheb_conv.py:119\u001b[0m, in \u001b[0;36mChebConv.__norm__\u001b[1;34m(self, edge_index, num_nodes, edge_weight, normalization, lambda_max, dtype, batch)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__norm__\u001b[39m(\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    111\u001b[0m     edge_index: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m     batch: OptTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    118\u001b[0m ):\n\u001b[1;32m--> 119\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mget_laplacian\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mnormalization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lambda_max \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric\\utils\\laplacian.py:69\u001b[0m, in \u001b[0;36mget_laplacian\u001b[1;34m(edge_index, edge_weight, normalization, dtype, num_nodes)\u001b[0m\n\u001b[0;32m     66\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[0;32m     68\u001b[0m row, col \u001b[38;5;241m=\u001b[39m edge_index[\u001b[38;5;241m0\u001b[39m], edge_index[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 69\u001b[0m deg \u001b[38;5;241m=\u001b[39m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# L = D - A.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     edge_index, _ \u001b[38;5;241m=\u001b[39m add_self_loops(edge_index, num_nodes\u001b[38;5;241m=\u001b[39mnum_nodes)\n",
      "File \u001b[1;32md:\\IT\\GitHub\\ITMO_subjects\\.venv\\lib\\site-packages\\torch_geometric\\utils\\_scatter.py:75\u001b[0m, in \u001b[0;36mscatter\u001b[1;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     74\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     78\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: index 2 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import add_self_loops, get_laplacian\n",
    "\n",
    "\n",
    "# Step 5: Define training loop\n",
    "def train_model(model, train_dataset, optimizer, loss_fn, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for snapshot in train_dataset:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Extract data from the snapshot\n",
    "            x, edge_index, edge_weight, y = snapshot.x, snapshot.edge_index, snapshot.edge_attr, snapshot.y\n",
    "            edge_index, edge_weight = add_self_loops(edge_index, edge_attr=edge_weight, num_nodes=num_nodes)\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(x, edge_index, edge_weight)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_dataset):.4f}\")\n",
    "\n",
    "# Step 6: Train the model\n",
    "train_model(model, train_dataset, optimizer, loss_fn, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
