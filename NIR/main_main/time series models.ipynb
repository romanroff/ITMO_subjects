{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import math\n",
    "import numpy.linalg as la\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Загрузка данных\n",
    "with h5py.File('../data/raw_data/METR-LA/METR-LA.h5', 'r') as file:\n",
    "    axis0 = file['df']['axis0'][:]              # Идентификаторы датчиков\n",
    "    axis1 = file['df']['axis1'][:]              # Метки времени\n",
    "    timestamps = pd.to_datetime(axis1)          # Преобразование меток времени в формат datetime\n",
    "    df_data = file['df']['block0_values'][:]    # Данные замеров скорости\n",
    "\n",
    "axis0 = [x.decode('utf-8') for x in axis0]\n",
    "metr_la = pd.DataFrame(df_data, index=timestamps, columns=axis0)\n",
    "metr_la = metr_la.iloc[:2016, :2]\n",
    "\n",
    "# Загрузка матрицы смежности\n",
    "with open('../data/raw_data/METR-LA/adj_METR-LA.pkl', 'rb') as file:\n",
    "    data = pickle.load(file, encoding='bytes')\n",
    "\n",
    "node_ids = [x.decode('utf-8') for x in data[0]]  # Получаем список id узлов из data[0]\n",
    "adj_matrix = data[2]  # Получаем матрицу смежности из data[2]\n",
    "metr_la_adj = pd.DataFrame(adj_matrix, index=node_ids, columns=node_ids)  # Создание DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобработка данных (ваша функция)\n",
    "def preprocess_data(data, time_len, rate, seq_len, pre_len):\n",
    "    data = np.array(data)\n",
    "    train_size = int(time_len * rate)\n",
    "    train_data = data[:train_size]\n",
    "    test_data = data[train_size:time_len]\n",
    "\n",
    "    trainX, trainY, testX, testY = [], [], [], []\n",
    "    for i in range(len(train_data) - seq_len - pre_len):\n",
    "        a = train_data[i: i + seq_len + pre_len]\n",
    "        trainX.append(a[0: seq_len])\n",
    "        trainY.append(a[seq_len: seq_len + pre_len])\n",
    "    for i in range(len(test_data) - seq_len - pre_len):\n",
    "        b = test_data[i: i + seq_len + pre_len]\n",
    "        testX.append(b[0: seq_len])\n",
    "        testY.append(b[seq_len: seq_len + pre_len])\n",
    "    return np.array(trainX), np.array(trainY), np.array(testX), np.array(testY)\n",
    "\n",
    "# Параметры\n",
    "time_len = len(metr_la)\n",
    "rate = 0.8\n",
    "seq_len = 12\n",
    "pre_len = 3\n",
    "\n",
    "# Предобработка\n",
    "trainX, trainY, testX, testY = preprocess_data(metr_la.values, time_len, rate, seq_len, pre_len)\n",
    "\n",
    "# Преобразование в тензоры PyTorch\n",
    "# trainX = torch.tensor(trainX, dtype=torch.float32).to(device)\n",
    "# trainY = torch.tensor(trainY, dtype=torch.float32).to(device)\n",
    "# testX = torch.tensor(testX, dtype=torch.float32).to(device)\n",
    "# testY = torch.tensor(testY, dtype=torch.float32).to(device)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Нормализация данных\n",
    "def normalize_data(trainX, testX):\n",
    "    scaler = MinMaxScaler()\n",
    "    # Обучение scaler на тренировочных данных и преобразование всех данных\n",
    "    trainX_shape = trainX.shape\n",
    "    testX_shape = testX.shape\n",
    "    trainX = scaler.fit_transform(trainX.reshape(-1, trainX_shape[-1])).reshape(trainX_shape)\n",
    "    testX = scaler.transform(testX.reshape(-1, testX_shape[-1])).reshape(testX_shape)\n",
    "    return trainX, testX, scaler\n",
    "\n",
    "# Нормализация trainX и testX\n",
    "trainX_norm, testX_norm, scaler_X = normalize_data(trainX, testX)\n",
    "trainY_norm, testY_norm, scaler_Y = normalize_data(trainY, testY)\n",
    "\n",
    "# Преобразование в тензоры PyTorch\n",
    "trainX_norm = torch.tensor(trainX_norm, dtype=torch.float32).to(device)\n",
    "trainY_norm = torch.tensor(trainY_norm, dtype=torch.float32).to(device)\n",
    "testX_norm = torch.tensor(testX_norm, dtype=torch.float32).to(device)\n",
    "testY_norm = torch.tensor(testY_norm, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, pre_len=3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.pre_len = pre_len\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_lstm, _ = self.lstm(x)  # h_lstm shape: [batch, seq_len, hidden_size]\n",
    "        # Используем последние pre_len скрытые состояния\n",
    "        h_lstm = h_lstm[:, -self.pre_len:, :]  # shape: [batch, pre_len, hidden_size]\n",
    "        out = self.fc(h_lstm)  # shape: [batch, pre_len, output_size]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, pre_len=3):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.pre_len = pre_len\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_gru, _ = self.gru(x)  # h_gru shape: [batch, seq_len, hidden_size]\n",
    "        # Используем последние pre_len скрытые состояния\n",
    "        h_gru = h_gru[:, -self.pre_len:, :]  # shape: [batch, pre_len, hidden_size]\n",
    "        out = self.fc(h_gru)  # shape: [batch, pre_len, output_size]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout=0.2, pre_len=3):\n",
    "        super(TCNModel, self).__init__()\n",
    "        self.pre_len = pre_len\n",
    "        self.tcn = nn.Sequential(\n",
    "            nn.Conv1d(input_size, num_channels, kernel_size, padding=(kernel_size - 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size, padding=(kernel_size - 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.fc = nn.Linear(num_channels, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (batch, seq_len, input_size) -> (batch, input_size, seq_len)\n",
    "        out = self.tcn(x)  # shape: [batch, num_channels, seq_len + kernel_size - 1]\n",
    "        # Используем последние pre_len временных шагов\n",
    "        out = out[:, :, -self.pre_len:]  # shape: [batch, num_channels, pre_len]\n",
    "        out = out.permute(0, 2, 1)  # shape: [batch, pre_len, num_channels]\n",
    "        out = self.fc(out)  # shape: [batch, pre_len, output_size]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainX, trainY, testX, testY, scaler_Y, epochs=50, lr=0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(trainX)  # outputs shape: [batch, pre_len, output_size]\n",
    "        loss = criterion(outputs, trainY)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Оценка модели\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions_norm = model(testX).cpu().numpy()  # predictions shape: [batch, pre_len, output_size]\n",
    "\n",
    "    # Восстановление данных\n",
    "    predictions = scaler_Y.inverse_transform(predictions_norm.reshape(-1, predictions_norm.shape[-1]))\n",
    "    predictions = predictions.reshape(predictions_norm.shape)  # Восстановление исходной формы\n",
    "    actuals = scaler_Y.inverse_transform(testY.cpu().numpy().reshape(-1, testY.shape[-1]))\n",
    "    actuals = actuals.reshape(testY.shape)  # Восстановление исходной формы\n",
    "\n",
    "    # Метрики\n",
    "    rmse = np.sqrt(mean_squared_error(actuals.reshape(-1), predictions.reshape(-1)))\n",
    "    mae = mean_absolute_error(actuals.reshape(-1), predictions.reshape(-1))\n",
    "    r2 = r2_score(actuals.reshape(-1), predictions.reshape(-1))\n",
    "    var = np.var(actuals - predictions)\n",
    "    return rmse, mae, r2, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры моделей\n",
    "input_size = trainX_norm.shape[2]  # Количество узлов (features)\n",
    "output_size = trainY_norm.shape[2]  # Количество узлов (features)\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "num_channels = 64\n",
    "kernel_size = 3\n",
    "pre_len = trainY_norm.shape[1]  # Длина предсказания (pre_len)\n",
    "\n",
    "# Создание моделей\n",
    "lstm_model = LSTMModel(input_size, hidden_size, output_size, num_layers, pre_len)\n",
    "gru_model = GRUModel(input_size, hidden_size, output_size, num_layers, pre_len)\n",
    "tcn_model = TCNModel(input_size, output_size, num_channels, kernel_size, pre_len=pre_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000], Loss: 0.0164\n",
      "Epoch [100/1000], Loss: 0.0158\n",
      "Epoch [150/1000], Loss: 0.0154\n",
      "Epoch [200/1000], Loss: 0.0149\n",
      "Epoch [250/1000], Loss: 0.0144\n",
      "Epoch [300/1000], Loss: 0.0141\n",
      "Epoch [350/1000], Loss: 0.0142\n",
      "Epoch [400/1000], Loss: 0.0137\n",
      "Epoch [450/1000], Loss: 0.0136\n",
      "Epoch [500/1000], Loss: 0.0135\n",
      "Epoch [550/1000], Loss: 0.0134\n",
      "Epoch [600/1000], Loss: 0.0140\n",
      "Epoch [650/1000], Loss: 0.0133\n",
      "Epoch [700/1000], Loss: 0.0132\n",
      "Epoch [750/1000], Loss: 0.0131\n",
      "Epoch [800/1000], Loss: 0.0130\n",
      "Epoch [850/1000], Loss: 0.0129\n",
      "Epoch [900/1000], Loss: 0.0127\n",
      "Epoch [950/1000], Loss: 0.0126\n",
      "Epoch [1000/1000], Loss: 0.0130\n",
      "Epoch [50/1000], Loss: 0.0140\n",
      "Epoch [100/1000], Loss: 0.0139\n",
      "Epoch [150/1000], Loss: 0.0138\n",
      "Epoch [200/1000], Loss: 0.0136\n",
      "Epoch [250/1000], Loss: 0.0135\n",
      "Epoch [300/1000], Loss: 0.0134\n",
      "Epoch [350/1000], Loss: 0.0133\n",
      "Epoch [400/1000], Loss: 0.0131\n",
      "Epoch [450/1000], Loss: 0.0130\n",
      "Epoch [500/1000], Loss: 0.0128\n",
      "Epoch [550/1000], Loss: 0.0127\n",
      "Epoch [600/1000], Loss: 0.0125\n",
      "Epoch [650/1000], Loss: 0.0123\n",
      "Epoch [700/1000], Loss: 0.0122\n",
      "Epoch [750/1000], Loss: 0.0121\n",
      "Epoch [800/1000], Loss: 0.0120\n",
      "Epoch [850/1000], Loss: 0.0119\n",
      "Epoch [900/1000], Loss: 0.0118\n",
      "Epoch [950/1000], Loss: 0.0118\n",
      "Epoch [1000/1000], Loss: 0.0116\n",
      "Epoch [50/1000], Loss: 0.0192\n",
      "Epoch [100/1000], Loss: 0.0174\n",
      "Epoch [150/1000], Loss: 0.0167\n",
      "Epoch [200/1000], Loss: 0.0160\n",
      "Epoch [250/1000], Loss: 0.0155\n",
      "Epoch [300/1000], Loss: 0.0150\n",
      "Epoch [350/1000], Loss: 0.0143\n",
      "Epoch [400/1000], Loss: 0.0138\n",
      "Epoch [450/1000], Loss: 0.0142\n",
      "Epoch [500/1000], Loss: 0.0137\n",
      "Epoch [550/1000], Loss: 0.0137\n",
      "Epoch [600/1000], Loss: 0.0134\n",
      "Epoch [650/1000], Loss: 0.0129\n",
      "Epoch [700/1000], Loss: 0.0129\n",
      "Epoch [750/1000], Loss: 0.0131\n",
      "Epoch [800/1000], Loss: 0.0127\n",
      "Epoch [850/1000], Loss: 0.0124\n",
      "Epoch [900/1000], Loss: 0.0124\n",
      "Epoch [950/1000], Loss: 0.0125\n",
      "Epoch [1000/1000], Loss: 0.0122\n",
      "Method       RMSE      MAE          R2      Var\n",
      "--------  -------  -------  ----------  -------\n",
      "LSTM      16.1527  8.30472  -0.0315222  256.857\n",
      "GRU       15.5656  6.90476   0.0421046  242.225\n",
      "TCN       14.1878  5.958     0.204176   200.743\n"
     ]
    }
   ],
   "source": [
    "# Обучение и оценка\n",
    "results = []\n",
    "for name, model in [(\"LSTM\", lstm_model), (\"GRU\", gru_model), (\"TCN\", tcn_model)]:\n",
    "    rmse, mae, r2, var = train_model(model, trainX_norm, trainY_norm, testX_norm, testY_norm, scaler_Y, epochs=1000)\n",
    "    results.append([name, rmse, mae, r2, var])\n",
    "\n",
    "# Вывод результатов\n",
    "print(tabulate(results, headers=[\"Method\", \"RMSE\", \"MAE\", \"R2\", \"Var\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0254\n",
      "Epoch [100/100], Loss: 0.0257\n",
      "Epoch [100/100], Loss: 0.0235\n",
      "Epoch [100/300], Loss: 0.0254\n",
      "Epoch [200/300], Loss: 0.0191\n",
      "Epoch [300/300], Loss: 0.0150\n",
      "Epoch [100/300], Loss: 0.0257\n",
      "Epoch [200/300], Loss: 0.0231\n",
      "Epoch [300/300], Loss: 0.0179\n",
      "Epoch [100/300], Loss: 0.0254\n",
      "Epoch [200/300], Loss: 0.0183\n",
      "Epoch [300/300], Loss: 0.0136\n",
      "Epoch [100/100], Loss: 0.0203\n",
      "Epoch [100/100], Loss: 0.0169\n",
      "Epoch [100/300], Loss: 0.0198\n",
      "Epoch [200/300], Loss: 0.0152\n",
      "Epoch [300/300], Loss: 0.0142\n",
      "Epoch [100/300], Loss: 0.0181\n",
      "Epoch [200/300], Loss: 0.0144\n",
      "Epoch [300/300], Loss: 0.0136\n",
      "Epoch [100/100], Loss: 0.0393\n",
      "Epoch [100/100], Loss: 0.0297\n",
      "Epoch [100/100], Loss: 0.0191\n",
      "Epoch [100/300], Loss: 0.0416\n",
      "Epoch [200/300], Loss: 0.0319\n",
      "Epoch [300/300], Loss: 0.0263\n",
      "Epoch [100/300], Loss: 0.0239\n",
      "Epoch [200/300], Loss: 0.0202\n",
      "Epoch [300/300], Loss: 0.0184\n",
      "Epoch [100/300], Loss: 0.0207\n",
      "Epoch [200/300], Loss: 0.0177\n",
      "Epoch [300/300], Loss: 0.0167\n",
      "Best params for LSTM: {'epochs': 100, 'hidden_size': 128, 'lr': 0.002, 'num_layers': 4}\n",
      "Best RMSE for LSTM: 15.218236923217773\n",
      "All results:\n",
      "params                                                                rmse      mae          r2      var\n",
      "-----------------------------------------------------------------  -------  -------  ----------  -------\n",
      "{'epochs': 100, 'hidden_size': 32, 'lr': 0.002, 'num_layers': 4}   16.0789  7.89272  -0.0221144  247.598\n",
      "{'epochs': 100, 'hidden_size': 64, 'lr': 0.002, 'num_layers': 4}   16.2077  7.84543  -0.0385493  249.913\n",
      "{'epochs': 100, 'hidden_size': 128, 'lr': 0.002, 'num_layers': 4}  15.2182  7.68555   0.0843815  225.744\n",
      "{'epochs': 300, 'hidden_size': 32, 'lr': 0.002, 'num_layers': 4}   15.7405  7.42288   0.0204636  247.564\n",
      "{'epochs': 300, 'hidden_size': 64, 'lr': 0.002, 'num_layers': 4}   15.521   7.7688    0.0475856  239.25\n",
      "{'epochs': 300, 'hidden_size': 128, 'lr': 0.002, 'num_layers': 4}  15.4941  6.90178   0.0508829  238.602\n",
      "Best params for GRU: {'epochs': 100, 'hidden_size': 64, 'lr': 0.002, 'num_layers': 4}\n",
      "Best RMSE for GRU: 14.919970512390137\n",
      "All results:\n",
      "params                                                                rmse      mae           r2      var\n",
      "-----------------------------------------------------------------  -------  -------  -----------  -------\n",
      "{'epochs': 100, 'hidden_size': 64, 'lr': 0.002, 'num_layers': 4}   14.92    7.51138   0.119921    218.04\n",
      "{'epochs': 100, 'hidden_size': 128, 'lr': 0.002, 'num_layers': 4}  14.9874  7.35921   0.111949    223.297\n",
      "{'epochs': 300, 'hidden_size': 64, 'lr': 0.002, 'num_layers': 4}   15.9124  7.62214  -0.00105631  253.203\n",
      "{'epochs': 300, 'hidden_size': 128, 'lr': 0.002, 'num_layers': 4}  15.8268  7.23444   0.00968844  249.671\n",
      "Best params for TCN: {'epochs': 300, 'kernel_size': 4, 'lr': 0.002, 'num_channels': 32}\n",
      "Best RMSE for TCN: 14.17673397064209\n",
      "All results:\n",
      "params                                                                  rmse      mae        r2      var\n",
      "-------------------------------------------------------------------  -------  -------  --------  -------\n",
      "{'epochs': 100, 'kernel_size': 4, 'lr': 0.002, 'num_channels': 32}   14.2222  7.19061  0.200319  202.228\n",
      "{'epochs': 100, 'kernel_size': 4, 'lr': 0.002, 'num_channels': 64}   14.4691  6.71854  0.172306  209.253\n",
      "{'epochs': 100, 'kernel_size': 4, 'lr': 0.002, 'num_channels': 128}  14.7944  6.41102  0.134678  218.628\n",
      "{'epochs': 300, 'kernel_size': 4, 'lr': 0.002, 'num_channels': 32}   14.1767  7.46115  0.205419  200.655\n",
      "{'epochs': 300, 'kernel_size': 4, 'lr': 0.002, 'num_channels': 64}   14.5933  6.47455  0.158039  212.964\n",
      "{'epochs': 300, 'kernel_size': 4, 'lr': 0.002, 'num_channels': 128}  14.5628  6.55654  0.161557  211.871\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Определение сетки параметров для каждой модели\n",
    "param_grid = {\n",
    "    \"LSTM\": {\n",
    "        \"hidden_size\": [32, 64, 128],\n",
    "        \"num_layers\": [4],\n",
    "        \"lr\": [0.002],\n",
    "        \"epochs\": [100, 300]\n",
    "    },\n",
    "    \"GRU\": {\n",
    "        \"hidden_size\": [64, 128],\n",
    "        \"num_layers\": [4],\n",
    "        \"lr\": [0.002],\n",
    "        \"epochs\": [100, 300]\n",
    "    },\n",
    "    \"TCN\": {\n",
    "        \"num_channels\": [32, 64, 128],\n",
    "        \"kernel_size\": [4],\n",
    "        \"lr\": [0.002],\n",
    "        \"epochs\": [100, 300]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Функция для Grid Search\n",
    "def grid_search(model_class, model_name, param_grid, trainX, trainY, testX, testY, scaler_Y):\n",
    "    best_rmse = float(\"inf\")\n",
    "    best_params = None\n",
    "    results = []\n",
    "\n",
    "    # Перебор всех комбинаций параметров\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        # print(f\"Testing {model_name} with params: {params}\")\n",
    "\n",
    "        # Создание модели\n",
    "        if model_name == \"LSTM\":\n",
    "            model = model_class(\n",
    "                input_size=trainX.shape[2],\n",
    "                hidden_size=params[\"hidden_size\"],\n",
    "                output_size=trainY.shape[2],\n",
    "                num_layers=params[\"num_layers\"],\n",
    "                pre_len=trainY.shape[1]\n",
    "            )\n",
    "        elif model_name == \"GRU\":\n",
    "            model = model_class(\n",
    "                input_size=trainX.shape[2],\n",
    "                hidden_size=params[\"hidden_size\"],\n",
    "                output_size=trainY.shape[2],\n",
    "                num_layers=params[\"num_layers\"],\n",
    "                pre_len=trainY.shape[1]\n",
    "            )\n",
    "        elif model_name == \"TCN\":\n",
    "            model = model_class(\n",
    "                input_size=trainX.shape[2],\n",
    "                output_size=trainY.shape[2],\n",
    "                num_channels=params[\"num_channels\"],\n",
    "                kernel_size=params[\"kernel_size\"],\n",
    "                pre_len=trainY.shape[1]\n",
    "            )\n",
    "\n",
    "        # Обучение модели\n",
    "        rmse, mae, r2, var = train_model(\n",
    "            model, trainX, trainY, testX, testY, scaler_Y,\n",
    "            epochs=params[\"epochs\"], lr=params[\"lr\"]\n",
    "        )\n",
    "\n",
    "        # Сохранение результатов\n",
    "        results.append({\n",
    "            \"params\": params,\n",
    "            \"rmse\": rmse,\n",
    "            \"mae\": mae,\n",
    "            \"r2\": r2,\n",
    "            \"var\": var\n",
    "        })\n",
    "\n",
    "        # Обновление лучших параметров\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_params = params\n",
    "\n",
    "    return best_params, best_rmse, results\n",
    "\n",
    "# Выполнение Grid Search для каждой модели\n",
    "best_results = {}\n",
    "for model_name, model_class in [(\"LSTM\", LSTMModel), (\"GRU\", GRUModel), (\"TCN\", TCNModel)]:\n",
    "    best_params, best_rmse, results = grid_search(\n",
    "        model_class, model_name, param_grid[model_name],\n",
    "        trainX_norm, trainY_norm, testX_norm, testY_norm, scaler_Y\n",
    "    )\n",
    "    best_results[model_name] = {\n",
    "        \"best_params\": best_params,\n",
    "        \"best_rmse\": best_rmse,\n",
    "        \"all_results\": results\n",
    "    }\n",
    "\n",
    "# Вывод лучших параметров и результатов\n",
    "for model_name, result in best_results.items():\n",
    "    print(f\"Best params for {model_name}: {result['best_params']}\")\n",
    "    print(f\"Best RMSE for {model_name}: {result['best_rmse']}\")\n",
    "    print(\"All results:\")\n",
    "    print(tabulate(result[\"all_results\"], headers=\"keys\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
