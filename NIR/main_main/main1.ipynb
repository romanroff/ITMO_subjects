{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle\n",
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "with h5py.File('../data/raw_data/PEMS-BAY/PEMS-BAY.h5', 'r') as file:\n",
    "\n",
    "    axis0 = file['speed']['axis0'][:]               # Идентификаторы датчиков\n",
    "    block0_items = file['speed']['block0_items'][:] # Идентификаторы датчиков\n",
    "    axis1 = file['speed']['axis1'][:]               # Метки времени\n",
    "    timestamps = pd.to_datetime(axis1)              # Преобразование меток времени в формат datetime\n",
    "    speed_data = file['speed']['block0_values'][:]  # Данные замеров скорости\n",
    "\n",
    "pems_bay = pd.DataFrame(speed_data, index=timestamps, columns=axis0)\n",
    "pems_bay = pems_bay[:2016]\n",
    "\n",
    "# Открытие .pkl файла\n",
    "with open('../data/raw_data/PEMS-BAY/adj_PEMS-BAY.pkl', 'rb') as file:\n",
    "    data = pickle.load(file, encoding='bytes')\n",
    "    \n",
    "node_ids = [x.decode('utf-8') for x in data[0]]                             # Получаем список id узлов из data[0]\n",
    "adj_matrix = data[2]                                                        # Получаем матрицу смежности из data[2]\n",
    "pems_bay_adj = pd.DataFrame(adj_matrix, index=node_ids, columns=node_ids)   # Создание DataFrame с использованием id узлов как индексов и названий колонок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2016 entries, 2017-01-01 00:00:00 to 2017-01-07 23:55:00\n",
      "Columns: 325 entries, 400001 to 414694\n",
      "dtypes: float64(325)\n",
      "memory usage: 5.0 MB\n"
     ]
    }
   ],
   "source": [
    "pems_bay.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метрики\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    return round(mae, 1), round(rmse, 1), round(mape, 1)\n",
    "\n",
    "# Функции для генерации фичей\n",
    "\n",
    "def generate_features(data):\n",
    "    features = data.copy()\n",
    "\n",
    "    # Скользящее среднее\n",
    "    rolling_mean = data.rolling(window=3).mean()\n",
    "    features = pd.concat([features, rolling_mean.add_suffix('_rolling_mean')], axis=1)\n",
    "\n",
    "    # # Преобразование Фурье (амплитуды)\n",
    "    # fft = np.fft.fft(data, axis=1)\n",
    "    # fft_amplitude = pd.DataFrame(np.abs(fft), index=data.index, columns=[f'fft_{i}' for i in range(data.shape[1])])\n",
    "    # features = pd.concat([features, fft_amplitude], axis=1)\n",
    "\n",
    "    # # Лаговые признаки\n",
    "    # for lag in range(1, 4):\n",
    "    #     lagged = data.shift(lag, axis=1).add_suffix(f'_lag{lag}')\n",
    "    #     features = pd.concat([features, lagged], axis=1)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Разделение данных на train, val, test\n",
    "\n",
    "def split_data(data, train_ratio=0.7, val_ratio=0.1):\n",
    "    n = data.shape[0]\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    train = data.iloc[:train_end, :]\n",
    "    val = data.iloc[train_end:val_end, :]\n",
    "    test = data.iloc[val_end:, :]\n",
    "    \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "adj_matrix = pems_bay_adj.copy()\n",
    "time_series = pems_bay.copy()\n",
    "\n",
    "# Генерация фичей\n",
    "original_features = time_series.copy()\n",
    "enhanced_features = generate_features(time_series)\n",
    "\n",
    "# Разделение данных\n",
    "train_orig, val_orig, test_orig = split_data(original_features)\n",
    "train_enh, val_enh, test_enh = split_data(enhanced_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение моделей\n",
    "results = {}\n",
    "\n",
    "# 1. Historical Average (HA)\n",
    "def historical_average(data):\n",
    "    return data.mean(axis=0).values\n",
    "\n",
    "ha_pred = historical_average(test_orig)\n",
    "results['HA_original'] = compute_metrics(test_orig.iloc[-1, :], ha_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ARIMA (на примере одного узла)\n",
    "def train_arima(data, node_index):\n",
    "    model = ARIMA(data.iloc[node_index, :-1], order=(5, 1, 0))\n",
    "    model_fit = model.fit()\n",
    "    pred = model_fit.forecast(steps=1)\n",
    "    return pred\n",
    "\n",
    "node_index = 0\n",
    "arima_pred = train_arima(train_orig, node_index)\n",
    "results['ARIMA_original'] = compute_metrics(test_orig.iloc[node_index, -1:], arima_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. SVR\n",
    "def train_svr(features, target):\n",
    "    model = SVR(kernel='rbf')\n",
    "    model.fit(features, target)\n",
    "    return model\n",
    "\n",
    "svr_model = train_svr(train_orig.T, val_orig.T.mean(axis=1))\n",
    "svr_pred = svr_model.predict(test_orig.T)\n",
    "results['SVR_original'] = compute_metrics(test_orig.iloc[:, -1], svr_pred)\n",
    "\n",
    "svr_model_enhanced = train_svr(train_enh.T, val_enh.T.mean(axis=1))\n",
    "svr_pred_enhanced = svr_model_enhanced.predict(test_enh.T)\n",
    "results['SVR_enhanced'] = compute_metrics(test_enh.iloc[:, -1], svr_pred_enhanced)\n",
    "\n",
    "# Результаты\n",
    "print(\"Results:\")\n",
    "for model, metrics in results.items():\n",
    "    print(f\"{model}: MAE={metrics[0]}, RMSE={metrics[1]}, MAPE={metrics[2]}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
