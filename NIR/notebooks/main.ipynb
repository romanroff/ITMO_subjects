{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Устанавливаем фиксированный сид\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # Для всех устройств CUDA\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Добавляем путь на уровень выше\n",
    "sys.path.append(str(Path(os.getcwd()).resolve().parent))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.load_data import load_data\n",
    "from utils.features import *\n",
    "from utils.feature_engineering import add_features\n",
    "from utils.training import run_grid_search\n",
    "from utils.data_processing import normalize_data, preprocess_data_multichannel\n",
    "from models.tgcn import TGCN, TGCNEmb\n",
    "from models.gru import GRUModel, GRUModelEmb\n",
    "from models.tcn import TCNModel, TCNModelEmb\n",
    "from models.dfdgcn import DFDGCN\n",
    "from models.lstm import LSTMModel\n",
    "from models.dcrnn import DCRNNModel\n",
    "from models.stconv import STConvModel\n",
    "\n",
    "datasets_metadata = {\n",
    "    'METR-LA': {'start_date': '2012-03-01', 'frequency': 5},\n",
    "    'PEMS-BAY': {'start_date': '2017-01-01', 'frequency': 5},\n",
    "    'PEMS03': {'start_date': '2018-01-09', 'frequency': 5},\n",
    "    'PEMS04': {'start_date': '2018-01-01', 'frequency': 5},\n",
    "    'PEMS07': {'start_date': '2012-05-01', 'frequency': 5},\n",
    "    'PEMS08': {'start_date': '2018-07-01', 'frequency': 5},\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/all_data/PEMS-BAY'\n",
    "data, metadata, adj_matrix = load_data(data_dir)\n",
    "data = data[:2016, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование в тензоры PyTorch\n",
    "A = torch.tensor(adj_matrix.values, dtype=torch.float32)\n",
    "edge_index = A.nonzero(as_tuple=False).t().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max(edge_index): 324\n",
      "min(edge_index): 0\n",
      "num_nodes: 325\n"
     ]
    }
   ],
   "source": [
    "print(f\"max(edge_index): {edge_index.max()}\")\n",
    "print(f\"min(edge_index): {edge_index.min()}\")\n",
    "print(f\"num_nodes: {adj_matrix.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Temp\\ipykernel_20236\\3312065895.py:11: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  index_train = pd.date_range(\n",
      "E:\\Temp\\ipykernel_20236\\3312065895.py:16: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  index_test = pd.date_range(\n"
     ]
    }
   ],
   "source": [
    "# Предобработка данных\n",
    "time_len = len(data)\n",
    "rate = 0.8\n",
    "seq_len = 12\n",
    "pre_len = 6\n",
    "train_len = int(time_len * rate)\n",
    "\n",
    "# Разделение данных на train и test\n",
    "train_data = data[:train_len]\n",
    "test_data = data[train_len:]\n",
    "index_train = pd.date_range(\n",
    "    start=datasets_metadata[metadata['name']]['start_date'],\n",
    "    periods=train_len,\n",
    "    freq=f'{datasets_metadata[metadata[\"name\"]][\"frequency\"]}T'\n",
    ")\n",
    "index_test = pd.date_range(\n",
    "    start=index_train[-1] + pd.Timedelta(minutes=datasets_metadata[metadata[\"name\"]][\"frequency\"]),\n",
    "    periods=len(test_data),\n",
    "    freq=f'{datasets_metadata[metadata[\"name\"]][\"frequency\"]}T'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расширение размерности для train и test\n",
    "train_data_extended = train_data[..., np.newaxis]\n",
    "test_data_extended = test_data[..., np.newaxis]\n",
    "\n",
    "# Определение функций\n",
    "feature_functions = {\n",
    "    'min': minute_index_feature,\n",
    "    # 'hour': hour_feature,\n",
    "    'weekday': weekday_index_feature,\n",
    "    # 'peak_hours': peak_hours_feature,\n",
    "    # 'mean': mean_feature,\n",
    "    # 'median': median_feature,\n",
    "    # 'std': std_feature,\n",
    "    # 'min': min_feature,\n",
    "    # 'max': max_feature,\n",
    "    # 'kurtosis': kurtosis_feature,\n",
    "    # 'skew': skew_feature,\n",
    "    # 'quantile': quantile_feature,\n",
    "    # 'rolling_mean': rolling_mean_feature,\n",
    "    # 'rolling_std': rolling_std_feature,\n",
    "    # 'rolling_min': rolling_min_feature,\n",
    "    # 'rolling_max': rolling_max_feature,\n",
    "}\n",
    "\n",
    "graph_feature_functions = {\n",
    "    'degree': degree_feature,\n",
    "    'degree_centrality': degree_centrality_feature,\n",
    "    'closeness_centrality': closeness_centrality_feature,\n",
    "    'betweenness_centrality': betweenness_centrality_feature,\n",
    "    'clustering_coefficient': clustering_coefficient_feature\n",
    "}\n",
    "\n",
    "# Добавление фичей для train\n",
    "train_data_extended = add_features(\n",
    "    train_data_extended,\n",
    "    feature_list=list(feature_functions.keys()),\n",
    "    feature_functions=feature_functions,\n",
    "    graph_feature_functions=graph_feature_functions,\n",
    "    index=index_train,\n",
    "    # adj_matrix=adj_matrix\n",
    ")\n",
    "\n",
    "# Добавление фичей для test\n",
    "test_data_extended = add_features(\n",
    "    test_data_extended,\n",
    "    feature_list=list(feature_functions.keys()),\n",
    "    feature_functions=feature_functions,\n",
    "    graph_feature_functions=graph_feature_functions,\n",
    "    index=index_test,\n",
    "    # adj_matrix=adj_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['Time series'] + list(feature_functions.keys()) + list(graph_feature_functions.keys())\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def corr_show(data, feature_names):\n",
    "    # 1. Преобразование данных в двумерный формат\n",
    "    num_timesteps, num_nodes, num_features = data.shape\n",
    "    data_2d = data.reshape(-1, num_features)  # Форма: (2016 * 2, C)\n",
    "\n",
    "    # 2. Создание DataFrame для удобства\n",
    "    df = pd.DataFrame(data_2d, columns=feature_names)\n",
    "\n",
    "    # 3. Вычисление матрицы корреляции\n",
    "    correlation_matrix = df.corr()\n",
    "    # correlation_matrix.dropna(how='all', inplace=True)\n",
    "    # correlation_matrix.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    # 4. Визуализация матрицы корреляции\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title('Correlation Matrix of Features')\n",
    "    plt.show()\n",
    "\n",
    "corr_show(train_data_extended, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def apply_pca_to_channels(data, channel_start, channel_end, pca_variance=0.95, standardize=True):\n",
    "    \"\"\"\n",
    "    Применяет PCA к указанным каналам данных и заменяет их на новые компоненты.\n",
    "\n",
    "    Параметры:\n",
    "        data (np.array): Исходные данные размерностью (samples, time_steps, features).\n",
    "        channel_start (int): Начальный индекс каналов для обработки.\n",
    "        channel_end (int): Конечный индекс каналов для обработки.\n",
    "        pca_variance (float): Доля дисперсии, которую нужно сохранить (по умолчанию 0.95).\n",
    "        standardize (bool): Применять ли стандартизацию перед PCA (по умолчанию True).\n",
    "\n",
    "    Возвращает:\n",
    "        data_new (np.array): Данные с замененными каналами.\n",
    "        pca: Объект PCA.\n",
    "        scaler: Объект StandardScaler (если стандартизация применялась).\n",
    "    \"\"\"\n",
    "    # Выделяем указанные каналы\n",
    "    channels = data[:, :, channel_start:channel_end]  # Размерность (samples, time_steps, n_channels)\n",
    "\n",
    "    # Преобразуем данные в (samples * time_steps, n_channels)\n",
    "    data_reshaped = channels.reshape(-1, channels.shape[-1])\n",
    "\n",
    "    # Стандартизация (если включена)\n",
    "    scaler = None\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        data_reshaped_scaled = scaler.fit_transform(data_reshaped)\n",
    "    else:\n",
    "        data_reshaped_scaled = data_reshaped\n",
    "\n",
    "    # Применяем PCA\n",
    "    pca = PCA(n_components=pca_variance)  # Сохраняем указанную долю дисперсии\n",
    "    data_pca = pca.fit_transform(data_reshaped_scaled)\n",
    "\n",
    "    # Преобразуем данные обратно в (samples, time_steps, n_components)\n",
    "    n_components = data_pca.shape[1]  # Количество компонент после PCA\n",
    "    data_pca_reshaped = data_pca.reshape(data.shape[0], data.shape[1], n_components)\n",
    "\n",
    "    # Заменяем каналы в исходном массиве на данные после PCA\n",
    "    data_new = data.copy()  # Создаем копию исходного массива\n",
    "    data_new[:, :, channel_start:channel_start + n_components] = data_pca_reshaped\n",
    "\n",
    "    # Если количество компонент меньше, чем исходное количество каналов, удаляем оставшиеся каналы\n",
    "    if n_components < (channel_end - channel_start):\n",
    "        data_new = np.delete(data_new, range(channel_start + n_components, channel_end), axis=2)\n",
    "\n",
    "    # Результаты\n",
    "    print(\"Исходная размерность данных:\", data.shape)\n",
    "    print(\"Исходная размерность каналов:\", channels.shape)\n",
    "    print(\"Размерность после PCA:\", data_pca_reshaped.shape)\n",
    "    print(\"Объясненная дисперсия:\", pca.explained_variance_ratio_.sum())\n",
    "    print(\"Размерность данных после замены:\", data_new.shape)\n",
    "    print()\n",
    "\n",
    "    return data_new, pca, scaler\n",
    "\n",
    "# Применяем PCA к каналам с 4 по 16 (индексы 4:16)\n",
    "train_data_extended_new, pca, scaler = apply_pca_to_channels(\n",
    "    data=train_data_extended,\n",
    "    channel_start=4,\n",
    "    channel_end=16,\n",
    "    pca_variance=4,\n",
    "    standardize=False\n",
    ")\n",
    "\n",
    "train_data_extended_new, pca, scaler = apply_pca_to_channels(\n",
    "    data=train_data_extended_new,\n",
    "    channel_start=7,\n",
    "    channel_end=12,\n",
    "    pca_variance=2,\n",
    "    standardize=False\n",
    ")\n",
    "\n",
    "# Применяем PCA к каналам с 4 по 16 (индексы 4:16)\n",
    "test_data_extended_new, pca, scaler = apply_pca_to_channels(\n",
    "    data=test_data_extended,\n",
    "    channel_start=4,\n",
    "    channel_end=16,\n",
    "    pca_variance=4,\n",
    "    standardize=False\n",
    ")\n",
    "\n",
    "test_data_extended_new, pca, scaler = apply_pca_to_channels(\n",
    "    data=test_data_extended_new,\n",
    "    channel_start=7,\n",
    "    channel_end=12,\n",
    "    pca_variance=2,\n",
    "    standardize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "n_channels = train_data_extended_new.shape[2]\n",
    "fig, axes = plt.subplots(n_channels, 1, figsize=(10, 4 * n_channels))\n",
    "if n_channels == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Построение гистограмм для каждого канала\n",
    "for i in range(n_channels):\n",
    "    channel_data = train_data_extended_new[:, :, i].reshape(-1, 1)\n",
    "    \n",
    "    sns.histplot(channel_data, kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f'Channel {i + 1}')\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainX  shape: torch.Size([1594, 12, 325, 3])\n",
      "TrainY  shape: torch.Size([1594, 6, 325])\n",
      "TestX   shape: torch.Size([386, 12, 325, 3])\n",
      "TestY   shape: torch.Size([386, 6, 325])\n"
     ]
    }
   ],
   "source": [
    "scale = True  # Флаг для включения/выключения нормализации\n",
    "\n",
    "# Обработка train и test для моделей\n",
    "trainX, trainY = preprocess_data_multichannel(train_data_extended, seq_len, pre_len)\n",
    "testX, testY = preprocess_data_multichannel(test_data_extended, seq_len, pre_len)\n",
    "\n",
    "# Нормализация данных (если scale = True)\n",
    "if scale:\n",
    "    trainX_norm, testX_norm, scaler_X = normalize_data(trainX, testX, method='norm')\n",
    "    trainY_norm, testY_norm, scaler_Y = normalize_data(trainY.reshape(-1, 1), testY.reshape(-1, 1), method='norm')\n",
    "    trainY_norm = trainY_norm.reshape(trainY.shape)\n",
    "    testY_norm = testY_norm.reshape(testY.shape)\n",
    "else:\n",
    "    # Если нормализация отключена, используем исходные данные\n",
    "    trainX_norm, testX_norm = trainX, testX\n",
    "    trainY_norm, testY_norm = trainY, testY\n",
    "    scaler_X, scaler_Y = None, None  # Scaler не используется\n",
    "\n",
    "# Преобразование в тензоры PyTorch\n",
    "trainX_norm = torch.tensor(trainX_norm, dtype=torch.float32)\n",
    "trainY_norm = torch.tensor(trainY_norm, dtype=torch.float32)\n",
    "testX_norm = torch.tensor(testX_norm, dtype=torch.float32)\n",
    "testY_norm = torch.tensor(testY_norm, dtype=torch.float32)\n",
    "\n",
    "print(f\"TrainX  shape: {trainX_norm.shape}\")    # [batch, seq_len, nodes, channels]\n",
    "print(f\"TrainY  shape: {trainY_norm.shape}\")    # [batch, pre_len, nodes, channels]\n",
    "print(f\"TestX   shape: {testX_norm.shape}\")     # [batch, seq_len, nodes, channels]\n",
    "print(f\"TestY   shape: {testY_norm.shape}\")     # [batch, pre_len, nodes, channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All results:\n",
      "model    params                                                                                                   rmse        mae         r2         var    training_time\n",
      "-------  --------------------------------------------------------------------------------------------------  ---------  ---------  ---------  ----------  ---------------\n",
      "GRU      {'batch_size': 32, 'epochs': 30, 'hidden_size': 32, 'lr': 0.005, 'num_layers': 4}                   0.0840138  0.0487244   0.116078  0.00646248         127.458\n",
      "GRUEmb   {'batch_size': 32, 'emb_dim': 12, 'epochs': 30, 'hidden_size': 32, 'lr': 0.005, 'num_layers': 4}    0.103417   0.0728306  -0.339358  0.00894154         134.922\n",
      "TCN      {'batch_size': 32, 'epochs': 30, 'kernel_size': 3, 'lr': 0.005, 'num_channels': 32}                 0.0383996  0.0203094   0.815343  0.00147007          73.7434\n",
      "TCNEmb   {'batch_size': 32, 'emb_dim': 12, 'epochs': 30, 'kernel_size': 3, 'lr': 0.005, 'num_channels': 32}  0.29007    0.232188   -9.53706   0.0341591          143.393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    # \"DFDGCN\": DFDGCN,\n",
    "    # \"LSTM\": LSTMModel,\n",
    "    \"GRU\": GRUModel,\n",
    "    \"GRUEmb\": GRUModelEmb,\n",
    "    \"TCN\": TCNModel,\n",
    "    \"TCNEmb\": TCNModelEmb,\n",
    "    # \"TGCN\": TGCN,\n",
    "    # \"TGCNEmb\": TGCNEmb,\n",
    "    # \"STConv\": STConvModel,\n",
    "    # \"DCRNN\": DCRNNModel\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    \"LSTM\": {\n",
    "        \"hidden_size\": [100],\n",
    "        \"num_layers\": [2],\n",
    "        \"lr\": [0.005],\n",
    "        \"epochs\": [30],\n",
    "        \"batch_size\": [32]\n",
    "    },\n",
    "    \"GRU\": {\n",
    "        \"hidden_size\": [32],\n",
    "        \"num_layers\": [4],\n",
    "        \"lr\": [0.005],\n",
    "        \"epochs\": [30],\n",
    "        \"batch_size\": [32]\n",
    "    },\n",
    "    \"GRUEmb\": {\n",
    "        \"hidden_size\": [32],\n",
    "        \"num_layers\": [4],\n",
    "        \"lr\": [0.005],\n",
    "        \"epochs\": [30],\n",
    "        \"batch_size\": [32],\n",
    "        \"emb_dim\": [12]\n",
    "    },\n",
    "    \"TCN\": {\n",
    "        \"num_channels\": [32],\n",
    "        \"kernel_size\": [3],\n",
    "        \"lr\": [0.005],\n",
    "        \"epochs\": [30],\n",
    "        \"batch_size\": [32]\n",
    "    },\n",
    "    \"TCNEmb\": {\n",
    "        \"num_channels\": [32],\n",
    "        \"kernel_size\": [3],\n",
    "        \"lr\": [0.005],\n",
    "        \"epochs\": [30],\n",
    "        \"batch_size\": [32],\n",
    "        \"emb_dim\": [12]\n",
    "    },\n",
    "    \"STConv\": {\n",
    "        \"hidden_channels\": [32],\n",
    "        \"kernel_size\": [3],\n",
    "        \"lr\": [0.005],\n",
    "        \"K\": [1],\n",
    "        \"epochs\": [30],\n",
    "        \"batch_size\": [32]\n",
    "    },\n",
    "    \"DFDGCN\": {\n",
    "        \"supports\": [[A]],\n",
    "        \"lr\": [0.005],\n",
    "        \"epochs\": [30],\n",
    "        \"batch_size\": [32]\n",
    "    },\n",
    "    \"TGCN\": {\n",
    "        \"hidden_dim\": [32],\n",
    "        'num_layers': [2],\n",
    "        \"lr\": [0.005],\n",
    "        \"epochs\": [30],\n",
    "        \"batch_size\": [32]\n",
    "    },\n",
    "    \"TGCNEmb\": {\n",
    "        \"hidden_dim\": [32],\n",
    "        'num_layers': [2],\n",
    "        \"lr\": [0.005],\n",
    "        \"epochs\": [30],\n",
    "        \"batch_size\": [32],\n",
    "        \"emb_dim\": [12]\n",
    "    }\n",
    "    # \"DCRNN\": {\n",
    "    #     \"hidden_channels\" : [32],\n",
    "    #     \"num_layers\" : [2],\n",
    "    #     \"K\" : [2],\n",
    "    #     \"lr\": [0.005],\n",
    "    #     \"epochs\": [30],\n",
    "    #     \"batch_size\": [32]\n",
    "    # }\n",
    "}\n",
    "\n",
    "# Запуск Grid Search\n",
    "run_grid_search(models, param_grids, trainX_norm, trainY_norm, testX_norm, testY_norm, scaler_Y=scaler_Y, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All results:\n",
      "model    params                                                                                                  rmse      mae         r2        var    training_time\n",
      "-------  --------------------------------------------------------------------------------------------------  --------  -------  ---------  ---------  ---------------\n",
      "GRU      {'batch_size': 32, 'epochs': 50, 'hidden_size': 32, 'lr': 0.002, 'num_layers': 4}                   11.148    6.43265  -1.29221   120.851            139.619\n",
      "GRUEmb   {'batch_size': 32, 'emb_dim': 12, 'epochs': 50, 'hidden_size': 32, 'lr': 0.002, 'num_layers': 4}     8.08585  4.63586  -0.205896   64.7468           355.726\n",
      "TCNEmb   {'batch_size': 32, 'emb_dim': 12, 'epochs': 50, 'kernel_size': 3, 'lr': 0.002, 'num_channels': 32}   2.5702   1.35783   0.878159    6.34032          352.116\n",
      "TCN      {'batch_size': 32, 'epochs': 50, 'kernel_size': 3, 'lr': 0.002, 'num_channels': 32}                  2.56459  1.31859   0.87869     6.57532          117.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "run_grid_search(models, param_grids, trainX_norm, trainY_norm, testX_norm, testY_norm, scaler_Y, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All results:\n",
    "# model    params                                                                                                rmse      mae        r2      var    training_time\n",
    "# -------  -------------------------------------------------------------------------------------------------  -------  -------  --------  -------  ---------------\n",
    "# TCNEmb   {'batch_size': 32, 'emb_dim': 4, 'epochs': 50, 'kernel_size': 3, 'lr': 0.002, 'num_channels': 32}  2.49434  1.29397  0.885246  6.21426          349.874\n",
    "# TCN      {'batch_size': 32, 'epochs': 50, 'kernel_size': 3, 'lr': 0.002, 'num_channels': 32}                2.58688  1.3467   0.876573  6.63827          115.958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TGCN Epoch [1/50]:   0%|          | 0/50 [00:00<?, ?it/s]                                                                                 All results:\n",
    "# model    params                                                                               rmse      mae        r2      var    training_time\n",
    "# -------  --------------------------------------------------------------------------------  -------  -------  --------  -------  ---------------\n",
    "# TGCN     {'batch_size': 32, 'epochs': 50, 'hidden_dim': 32, 'lr': 0.002, 'num_layers': 2}  126.655  100.314  0.266563  16037.5          231.727"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All results:\n",
    "# model    params                                                                                                  rmse      mae        r2       var    training_time\n",
    "# -------  --------------------------------------------------------------------------------------------------  --------  -------  --------  --------  ---------------\n",
    "# DFDGCN   {'batch_size': 32, 'epochs': 30, 'lr': 0.002, 'supports': [tensor([[0., 1., 0.,  ..., 0., 0., 1.],   33.8138  22.3837  0.947723   1120.15         1645.2\n",
    "#                  [1., 0., 0.,  ..., 0., 0., 0.],\n",
    "#                  [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "#                  ...,\n",
    "#                  [0., 0., 0.,  ..., 0., 1., 0.],\n",
    "#                  [0., 0., 0.,  ..., 1., 0., 0.],\n",
    "#                  [1., 0., 0.,  ..., 0., 0., 0.]])]}\n",
    "# LSTM     {'batch_size': 32, 'epochs': 50, 'hidden_size': 32, 'lr': 0.002, 'num_layers': 4}                   121.732   95.3205  0.322463  14724.4           154.29\n",
    "# GRU      {'batch_size': 32, 'epochs': 50, 'hidden_size': 32, 'lr': 0.002, 'num_layers': 4}                   120.737   94.3762  0.3335    14568.4           317.023\n",
    "# TCN      {'batch_size': 32, 'epochs': 50, 'kernel_size': 3, 'lr': 0.002, 'num_channels': 32}                  42.8252  28.1331  0.916147   1830.15          128.45\n",
    "\n",
    "# All results:\n",
    "# model    params                                                                                                                         rmse      mae        r2      var    training_time\n",
    "# -------  --------------------------------------------------------------------------------------------------------------------------  -------  -------  --------  -------  ---------------\n",
    "# DFDGCN   {'batch_size': 32, 'epochs': 30, 'lr': 0.002, 'supports': [tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],  1.98562  1.09549  0.927607   3.9088         1574.38\n",
    "#                  [0.0000, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
    "#                  [0.0000, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
    "#                  ...,\n",
    "#                  [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.9605, 0.6061],\n",
    "#                  [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.7730],\n",
    "#                  [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]])]}\n",
    "# LSTM     {'batch_size': 32, 'epochs': 50, 'hidden_size': 32, 'lr': 0.002, 'num_layers': 4}                                           6.7872   4.06529  0.154162  42.553           141.494\n",
    "# GRU      {'batch_size': 32, 'epochs': 50, 'hidden_size': 32, 'lr': 0.002, 'num_layers': 4}                                           6.61048  3.92131  0.197637  43.5846          139.021\n",
    "# TCN      {'batch_size': 32, 'epochs': 50, 'kernel_size': 3, 'lr': 0.002, 'num_channels': 32}                                         2.32921  1.28585  0.900385   5.4208          112.511\n",
    "\n",
    "# All results:\n",
    "# model    params                                                                                                                         rmse      mae        r2       var    training_time\n",
    "# -------  --------------------------------------------------------------------------------------------------------------------------  -------  -------  --------  --------  ---------------\n",
    "# DFDGCN   {'batch_size': 32, 'epochs': 30, 'lr': 0.002, 'supports': [tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],  2.08501  1.25713  0.920178   3.93405         1576.2\n",
    "#                  [0.0000, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
    "#                  [0.0000, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
    "#                  ...,\n",
    "#                  [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.9605, 0.6061],\n",
    "#                  [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.7730],\n",
    "#                  [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]])]}\n",
    "# LSTM     {'batch_size': 32, 'epochs': 50, 'hidden_size': 32, 'lr': 0.002, 'num_layers': 4}                                           6.56797  3.92606  0.207922  41.9298           141.443\n",
    "# GRU      {'batch_size': 32, 'epochs': 50, 'hidden_size': 32, 'lr': 0.002, 'num_layers': 4}                                           6.45581  3.84676  0.234744  41.4944           138.62\n",
    "# TCN      {'batch_size': 32, 'epochs': 50, 'kernel_size': 3, 'lr': 0.002, 'num_channels': 32}                                         2.35675  1.27149  0.898016   5.50779          114.759"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All results:\n",
    "# model    params                                                                                  rmse      mae        r2       var\n",
    "# -------  -----------------------------------------------------------------------------------  -------  -------  --------  --------\n",
    "# LSTM     {'batch_size': 64, 'epochs': 50, 'hidden_size': 32, 'lr': 0.002, 'num_layers': 4}    6.83275  3.89527  0.142772  43.9322\n",
    "# GRU      {'batch_size': 64, 'epochs': 50, 'hidden_size': 32, 'lr': 0.002, 'num_layers': 4}    6.55248  3.87363  0.211654  42.8969\n",
    "# TCN      {'batch_size': 64, 'epochs': 50, 'kernel_size': 3, 'lr': 0.002, 'num_channels': 32}  2.29599  1.28208  0.903207   5.26139\n",
    "\n",
    "# All results:\n",
    "# model    params                                                                                  rmse      mae        r2       var\n",
    "# -------  -----------------------------------------------------------------------------------  -------  -------  --------  --------\n",
    "# LSTM     {'batch_size': 64, 'epochs': 50, 'hidden_size': 32, 'lr': 0.002, 'num_layers': 4}    6.65039  3.93296  0.187919  44.2228\n",
    "# GRU      {'batch_size': 64, 'epochs': 50, 'hidden_size': 32, 'lr': 0.002, 'num_layers': 4}    6.66022  3.94188  0.185517  44.3371\n",
    "# TCN      {'batch_size': 64, 'epochs': 50, 'kernel_size': 3, 'lr': 0.002, 'num_channels': 32}  2.41745  1.39945  0.892695   5.53997\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
