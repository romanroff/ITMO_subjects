{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Работа с тензорами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Сумма тензоров:\n",
      " tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "\n",
      "Транспонирование тензора:\n",
      " tensor([[1., 3.],\n",
      "        [2., 4.]])\n",
      "\n",
      "Матричное умножение тензоров:\n",
      " tensor([[19., 22.],\n",
      "        [43., 50.]])\n"
     ]
    }
   ],
   "source": [
    "# Создание тензоров\n",
    "tensor_a = torch.tensor([[1.0, 2.0], \n",
    "                         [3.0, 4.0]])\n",
    "\n",
    "tensor_b = torch.tensor([[5.0, 6.0], \n",
    "                         [7.0, 8.0]])\n",
    "\n",
    "# Сложение тензоров\n",
    "tensor_sum = tensor_a + tensor_b\n",
    "print(\"\\nСумма тензоров:\\n\", tensor_sum)\n",
    "\n",
    "# Транспонирование тензора\n",
    "tensor_transpose = tensor_a.T\n",
    "print(\"\\nТранспонирование тензора:\\n\", tensor_transpose)\n",
    "\n",
    "# Матричное умножение тензоров\n",
    "tensor_matmul = torch.matmul(tensor_a, tensor_b)\n",
    "print(\"\\nМатричное умножение тензоров:\\n\", tensor_matmul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Градиенты "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиент для функции $ f(x) = x^2 $\n",
    "\n",
    "1. Аналитическое вычисление градиента\n",
    "\n",
    "    Для функции $ f(x) = x^2 $, аналитический градиент можно вычислить с помощью производной:\n",
    "    $$\n",
    "    f'(x) = \\frac{d}{dx} (x^2) = 2x\n",
    "    $$\n",
    "    То есть, производная функции $ x^2 $ в точке $ x = a $ будет равна $ 2a $.\n",
    "\n",
    "2. Вычисление градиента с PyTorch\n",
    "\n",
    "    В PyTorch, используя автоматическое дифференцирование, можно вычислить градиент функции в произвольной точке, используя метод `backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Градиент функции y=x^2 при x=3: 6.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "\n",
    "# Печатаем значение градиента dy/dx в точке x=3\n",
    "print(f\"Градиент функции y=x^2 при x=3: {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пояснение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тензоры позволяют представлять входные данные (например, изображения, текст, аудио) и обучаемые параметры (например, веса нейронной сети) в формате матриц. Тензоры используются буквально везде в глубоком обучении, так как каждый шаг - это вычисление, выделение куска тензора и так далее.\n",
    "\n",
    "Градиенты используются для оптимизации моделей. С помощью обратного распространения ошибки градиенты позволяют настраивать параметры модели, минимизируя функцию потерь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1: Введение в сверточные нейронные сети (CNN) для классификации изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задачи:\n",
    "1.\tЗагрузить датасет CIFAR-10, выполнить нормализацию и подготовить данные для обучения.\n",
    "2.\tОпределить архитектуру сети с двумя сверточными слоями, пулингом и полносвязными слоями.\n",
    "3.\tНаписать тренировочный цикл, запустить обучение и отслеживать точность на обучающей и тестовой выборках.\n",
    "4.\tВизуализировать функцию потерь и оценить точность, объяснив, какие улучшения можно внести для повышения производительности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Определяем трансформации для нормализации данных\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Нормализация\n",
    "])\n",
    "\n",
    "# Загружаем CIFAR-10 с помощью DataLoader\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  # Первый сверточный слой\n",
    "        self.pool = nn.MaxPool2d(2, 2)               # MaxPooling\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1) # Второй сверточный слой\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 120)        # Полносвязный слой\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)                 # Выходной слой для 10 классов\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))         # Сверточный слой + пулинг\n",
    "        x = self.pool(F.relu(self.conv2(x)))         # Сверточный слой + пулинг\n",
    "        x = x.view(-1, 32 * 8 * 8)                   # Вытягиваем тензор\n",
    "        x = F.relu(self.fc1(x))                      # Полносвязный слой\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)                              # Выходной слой\n",
    "        return x\n",
    "\n",
    "net = SimpleCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Определяем функцию потерь и оптимизатор\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Тренировочный цикл\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()                   # Обнуляем градиенты\n",
    "        outputs = net(inputs)                   # Прямой проход\n",
    "        loss = criterion(outputs, labels)       # Вычисление функции потерь\n",
    "        loss.backward()                         # Обратное распространение\n",
    "        optimizer.step()                        # Шаг оптимизатора\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:                      # Выводим каждые 200 mini-batches\n",
    "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 200:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Обучение завершено.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "# Обновляем тренировочный цикл, чтобы сохранять историю потерь\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 200 == 199:\n",
    "            loss_history.append(running_loss / 200)\n",
    "            running_loss = 0.0\n",
    "\n",
    "# Визуализация функции потерь\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2: Классификация изображений с предобработкой и аугментацией данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задачи:\n",
    "1.\tЗагрузить и предобработать CIFAR-10, Fashion-MNIST и SVHN, применяя различные техники аугментации.\n",
    "2.\tОбучить модель CNN на каждом из датасетов, сравнить точность и функцию потерь.\n",
    "3.\tОценить влияние аугментации на результат и описать, какие методы лучше подходят для каждого датасета.\n",
    "4.\tСделать выводы о том, как предобработка помогает улучшить обобщающие способности модели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3: Оптимизация архитектуры CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задачи:\n",
    "1.\tСоздать три варианта архитектуры CNN с разным количеством фильтров и размерами ядер.\n",
    "2.\tДобавить и настроить Batch Normalization и Dropout для предотвращения переобучения.\n",
    "3.\tПровести сравнение моделей по точности и функции потерь на тестовой выборке.\n",
    "4.\tОписать, какой подход лучше работает для данной задачи и почему, предлагая возможные улучшения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 4: Оптимизация гиперпараметров с использованием нестандартных методов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задачи:\n",
    "1.\tОбучить модель с циклическим и адаптивным темпами обучения, сравнить результаты.\n",
    "2.\tПротестировать разные значения batch size и настроить накопление градиентов для экономии памяти.\n",
    "3.\tСравнить модели с разным количеством фильтров и слоев, чтобы оценить влияние каждого гиперпараметра.\n",
    "4.\tНа основе полученных результатов предложить оптимальные настройки гиперпараметров для повышения точности и стабильности обучения.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
